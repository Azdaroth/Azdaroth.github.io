<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Database | Karol Galanciak - Ruby on Rails and Ember.js consultant]]></title>
  <link href="https://karolgalanciak.com/blog/categories/database/atom.xml" rel="self"/>
  <link href="https://karolgalanciak.com/"/>
  <updated>2018-08-19T20:55:17+02:00</updated>
  <id>https://karolgalanciak.com/</id>
  <author>
    <name><![CDATA[Karol Galanciak]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Indexes on Rails: How to make the most of your Postgres database]]></title>
    <link href="https://karolgalanciak.com/blog/2018/08/19/indexes-on-rails-how-to-make-the-most-of-your-postgres-database/"/>
    <updated>2018-08-19T20:00:00+02:00</updated>
    <id>https://karolgalanciak.com/blog/2018/08/19/indexes-on-rails-how-to-make-the-most-of-your-postgres-database</id>
    <content type="html"><![CDATA[<p>Optimizing <strong>database queries</strong> is arguably one of the fastest ways to improve the <strong>performance</strong> of the Rails applications. There are multiple ways how you can approach it, depending on the kind of a problem. <strong>N+1 queries</strong> seem to be a pretty common issue, which is, fortunately, <strong>easy to address</strong>. However, sometimes you have some relatively <strong>simple-looking queries</strong> that seem to take way longer than they should be, indicating that they might require some optimization. The best way to improve such queries is adding a <strong>proper index</strong>.</p>

<p>But what does &ldquo;proper index&rdquo; mean? How to figure out what kind of index is exactly needed for a given query? Here are some essential facts and tips that should cover a majority of the queries you may encounter and make your database no longer a bottleneck.</p>

<!--more-->


<h2>Why index at all?</h2>

<p>Simple &ndash; to have faster queries. But why are indexes faster? The alternative to index is a sequential scanning of the entire table. That might not sound like a bad idea, but imagine you are performing a search over a huge table. What would be the fastest way to retrieve all records you are looking for &ndash; by scanning the entire table, or maybe having a way to store a subset of the records, based on some specific criteria, and then retrieve them from that place? Obviously, it&rsquo;s the second option. And that&rsquo;s roughly how indexes work.</p>

<p>As trivial as it sounds, there is a valuable lesson to learn from it: to achieve a good performance, the index must be selective enough. And the more specific you will be about those criteria, the better.</p>

<h2>Index Types</h2>

<p>Although Postgres by defaults creates <code>B-Tree</code> index when using <code>CREATE INDEX</code> command, there are a couple of more indexes that will be certainly useful in many use cases. Let&rsquo;s check them all out:</p>

<h3>B-Tree Index</h3>

<p><code>B-Tree</code> is a self-balancing tree data structure which keeps data ordered and easy to search. This index is appropriate for equality and range queries (using operators like <code>&gt;=</code>,  <code>&lt;</code> etc.) and will work great with text, timestamp and number fields.</p>

<p>B-Tree indexes are a reasonable default for most of the queries, but not for all of them. The limitation comes from the underlying structure. Discussing the details of the B-Tree data structure itself is beyond the scope of this article; nevertheless, it&rsquo;s worth keeping in mind that it&rsquo;s a similar data structure to a binary search tree, which has meaningful consequences on what can be indexed with it and how. We will get back to a couple of examples later.</p>

<h3>Hash Index</h3>

<p>Before Postgres 10, the usage of hash indexes was discouraged since they used to be not WAL-logged. Fortunately, it&rsquo;s changed in Postgres 10, and we can use them safely without worrying about rebuilding the index if something goes wrong with our database that would cause a crash. The use cases where hash indexes are useful are very limited, as they work only for equality, but they are a bit more efficient for this kind of queries comparing to b-tree indexes. If you store tokens for example and perform lookups by the token value, hash indexes would be a good way to optimize such queries.</p>

<h3>BRIN Index (Block Range Index)</h3>

<p>BRIN indexes were introduced in Postgres 9.5 which make them a pretty new addition. They tend to work very well for the large sets of ordered data, e.g., statistical data collected with timestamps which are later filtered by the time range. They will perform better than b-tree indexes in such case, although the difference won&rsquo;t be drastic. However, the different of the size of the index will be huge &ndash; BRIN index can be smaller by literally few orders of magnitude comparing to b-tree index.</p>

<h3>GIN Index (Generalized Inverted Index)</h3>

<p>GIN Indexes are the perfect choice for &ldquo;composite values&rdquo; where you perform a query which looks for an element within such &ldquo;composite&rdquo;. That is the index you will most likely want to use for <code>jsonb</code>, <code>array</code> or <code>hstore</code> data structures. They are also an excellent choice for full-text search.</p>

<h3>GiST Index (Generalized Inverted Seach Tree Index)</h3>

<p>GiST Indexes will be a good choice when the records overlap values under the same column. They are commonly used for geometry types and full-text search as well. The difference between GIN and GiST Index when it comes to full-text search is that GiST Index will be less taxing on writes comparing to GIN (as it is faster to build). But since it&rsquo;s a lossy index, there might be some extra overhead involved for reads, which makes GIN index a better choice when you mostly care about reads optimization.</p>

<h2>Optimizing Queries</h2>

<p>Here are some tips that should help you with the majority of the queries:</p>

<h3>Start with EXPLAIN</h3>

<p>With enough experience and knowledge of your app, you will develop an intuition about indexes and where they might be useful, long before having performance problems with queries. Until that happens, it&rsquo;s essential to understand how Postgres is going to execute these queries. The best tool for that purpose is using <code>EXPLAIN</code> command, which will show the execution plan generated by the query planner. <code>ActiveRecord</code> provides a convenient method &ndash; <code>explain</code> &ndash; that you can use on collections to get the query plan:</p>

<p>```</p>

<blockquote><p>Order.where(customer_id: 1).explain
  Order Load (13.8ms)  SELECT &ldquo;orders&rdquo;.<em> FROM &ldquo;orders&rdquo; WHERE &ldquo;orders&rdquo;.&ldquo;customer_id&rdquo; = $1  [[&ldquo;customer_id&rdquo;, 1]]
=> EXPLAIN for: SELECT &ldquo;orders&rdquo;.</em> FROM &ldquo;orders&rdquo; WHERE &ldquo;orders&rdquo;.&ldquo;customer_id&rdquo; = $1 [[&ldquo;customer_id&rdquo;, 1]]</p>

<pre><code>                                       QUERY PLAN
</code></pre>

<hr />

<p> Index Scan using index_orders_on_customer_id on orders  (cost=0.15..19.62 rows=50 width=1417)
   Index Cond: (customer_id = 1)
(2 rows)
```</p></blockquote>

<h3>How to tell a good query plan from a bad one?</h3>

<p>This is not that simple as it sounds, as the sequential scan can be sometimes more efficient than using an index, especially if not kept in memory, but stored entirely on disk, and even worse, an HDD one. Usually, a preferable query plan is the one that looks simpler and utilizes the least possible number of indexes, which means that it&rsquo;s better to use one index instead of two of them, like in the following example:</p>

<p>```</p>

<blockquote><p>Product.where(warehouse_id: 1).where(category_id: 1).explain
  Product Load (0.5ms)  SELECT &ldquo;products&rdquo;.<em> FROM &ldquo;products&rdquo; WHERE &ldquo;products&rdquo;.&ldquo;warehouse_id&rdquo; = $1 AND &ldquo;products&rdquo;.&ldquo;category_id&rdquo; = $2  [[&ldquo;warehouse_id&rdquo;, 1], [&ldquo;category_id&rdquo;, 1]]
=> EXPLAIN for: SELECT &ldquo;products&rdquo;.</em> FROM &ldquo;products&rdquo; WHERE &ldquo;products&rdquo;.&ldquo;warehouse_id&rdquo; = $1 AND &ldquo;products&rdquo;.&ldquo;category_id&rdquo; = $2 [[&ldquo;warehouse_id&rdquo;, 1], [&ldquo;category_id&rdquo;, 1]]</p>

<pre><code>                                                   QUERY PLAN
</code></pre>

<hr />

<p> Bitmap Heap Scan on products  (cost=9.08..13.10 rows=1 width=1417)
   Recheck Cond: ((warehouse_id = 1) AND (category_id = 1))
   &ndash;>  BitmapAnd  (cost=9.08..9.08 rows=1 width=0)</p>

<pre><code>     -&gt;  Bitmap Index Scan on index_products_on_warehouse_id_and_name_and_something_else  (cost=0.00..4.31 rows=5 width=0)
           Index Cond: (warehouse_id = 1)
     -&gt;  Bitmap Index Scan on index_products_on_category_id  (cost=0.00..4.52 rows=50 width=0)
           Index Cond: (category_id = 1)
</code></pre>

<p>(7 rows)
```</p></blockquote>

<p>It doesn&rsquo;t necessarily mean that this query plan is a bad one &ndash; it could be totally the case that such query is fast enough. However, if read speed is more important for us than the index size and extra overhead on writes which will make them slower, the best way to deal with such query would be adding a compound index on both <code>warehouse_id</code> and <code>category_id</code>.</p>

<p>One statement that is especially worth keeping an eye on (besides <code>Seq Scan</code> which stands for a sequential scan) is <code>Filter</code> statement which indicates that the records required extra filtering and the index was not enough. Here is one example:</p>

<p><code>
 Index Scan using index_products_on_category_id on products
   Index Cond: (category = 1)
   Filter: (created_at = '2018-08-11'::date)
</code></p>

<p>Ideally, <code>created_at</code> part would appear in <code>Index Cond</code> and be fully covered by the index. Usually, adding a compound index on multiple columns solves the issue which in this example would mean having an index on both <code>category_id</code> and <code>created_at</code>, not only on <code>category_id</code>.</p>

<h3>Sequence of the columns in B-Tree index does matter</h3>

<p>The sequence of the columns in a multi-column index is critical. Imagine that you created a following index: <code>create_index :tag_items, [:taggable_type, :taggable_id]</code> and want to perform a couple of queries. For sure this index is going to be efficient for searching by both <code>taggable_type</code> and <code>taggable_id</code>. It will also work great for search by <code>taggable_type</code>. It won&rsquo;t, however, be efficient when performing a search just by <code>taggable_id</code>. The reason why it is like that is quite simple though &ndash; try to imagine how the data would be stored in a hypothetical B-Tree. First, the nodes will be organized based on the leftmost column and then, by another one. Traversing such tree when you do a search based on <code>taggable_type</code> or both <code>taggable_type</code> and <code>taggable_id</code> will be simple. However, you can&rsquo;t do the same with just <code>taggable_id</code>. Postgres might use this index anyway as it might turn out to be still more efficient than a sequential scan, but this is going to be suboptimal. If it happens that you need to perform queries by <code>taggable_id</code> only, it would be a good idea to add a separate index on that field.</p>

<h3>Unique Indexes</h3>

<p>The biggest need behind unique indexes is ensuring data integrity (since most uniqueness validations, including ActiveRecord one, don&rsquo;t enforce anything and are more useful for having a nice error message and not raising an exception than for data integrity). However, a nice side effect of a unique index is also a better performance comparing to a non-unique one.</p>

<h3>Partial Indexes</h3>

<p>Imagine that you have some Articles in your application and you want to add <code>published_at</code> datetime field indicating whether and when the article was published, and then, filter published articles by a given author. We can most likely expect a need for an index on <code>author_id</code> column in such case. What about our second condition? We could for sure add a compound index on both <code>author_id</code> and <code>published_at</code>. However, there is a better choice. We could add a partial index for <code>author_id</code> which covers only published articles, i.e., covers <code>WHERE published_at IS NOT NULL</code>!</p>

<p>Fortunately, this is supported by Rails (although writing a SQL command wouldn&rsquo;t be that difficult), we just need to use <code>where</code> option for that:</p>

<p><code>rb
add_column :articles, :author_id, where: "published_at IS NOT NULL"
</code></p>

<h3>Expression Indexes</h3>

<p>Imagine that you need to search users by their first name which comes from some input provided by a user. However, to avoid issues with figuring out whether the name provided by a user starts with a capital letter or not or how the names were stored in the database in the first place, you perform a query like this:</p>

<p><code>rb
User.where("lower(first_name) = ?", name.downcase)
</code></p>

<p>This query will obviously work, however, if you have a lot of users, a query plan will indicate that it is suboptimal and instead of seeing something like <code>Index Scan using index_users_on_first_name on users</code>, you will see <code>Seq Scan on users</code>.</p>

<p>There is no need to worry though. Postgres allows creating expression indexes where you can apply some functions, which in our case is <code>lower</code>. A proper index for this scenario would need to be created that way:</p>

<p><code>rb
add_index :users, "lower(first_name)", name: "index_users_on_lower_first_name"
</code></p>

<h3>Optimizing LIKE queries</h3>

<p>Optimizing queries with <code>LIKE</code> clause is simple; you just need to remember about two things:</p>

<ol>
<li>Forget about B-Tree Index for this case.</li>
<li>Take advantage of trigram matching provided by <a href="https://www.postgresql.org/docs/10/static/pgtrgm.html">pg_trgm</a> extension.</li>
</ol>


<p>To avoid sequential scans and utilize index that will drastically improve the performance of this kind of queries, enable the extension and create a GIN or GiST index:</p>

<p><code>
execute "CREATE EXTENSION pg_trgm;"
execute "CREATE INDEX CONCURRENTLY index_products_on_description_trigram ON clients USING gin(description gin_trgm_ops);"
</code></p>

<p>Thanks to this index, this is a query plan you might expect when filtering <code>Products</code> by <code>descriptions</code> containing some text, with wildcards on both the beginning and the end:</p>

<p>```
EXPLAIN for: SELECT &ldquo;products&rdquo;.* FROM &ldquo;products&rdquo; WHERE (products.description ILIKE &lsquo;%some text with wildacards%&rsquo;)</p>

<pre><code>                                            QUERY PLAN
</code></pre>

<hr />

<p> Bitmap Heap Scan on products
   Recheck Cond: (description ~~* &lsquo;%some text with wildacards%&rsquo;::text)
   &ndash;>  Bitmap Index Scan on index_products_on_description_trigram</p>

<pre><code>     Index Cond: (description ~~* '%some text with wildacards%'::text)
</code></pre>

<p>```</p>

<h3>Ordering</h3>

<p>B-tree indexes are sorted in an ascending order which we can use to our advantage to avoid performing sorting in memory. However, we also need to keep in mind the limitation of the data structure itself. A rule of thumb for efficient ordering would be: order by the same columns you perform filtering by. It is going to be the case by default when you don&rsquo;t explicitly add any <code>ORDER</code> clause since the indexes are ordered. But if it happens that you need to apply different ordering criteria, you can take advantage of <code>order</code> option and explicitly specify the order:</p>

<p><code>
add_index :products, :created_at, order: { created_at: :desc }
</code></p>

<h3>Adding indexes concurrently</h3>

<p>The way how the indexes are added doesn&rsquo;t impact the performance once they are created; however, it&rsquo;s good to keep in mind that just simple <code>CREATE INDEX</code> will block concurrent writes (inserts, updates, and deletes) until it&rsquo;s finished. It can lead to some issues, including deadlocks, especially when the index is getting created for a huge table under massive write operations.</p>

<p>To prevent such a problem, it&rsquo;s worth creating indexes <a href="https://www.postgresql.org/docs/10/static/sql-createindex.html#SQL-CREATEINDEX-CONCURRENTLY">concurrently</a> instead. You can do that in Rails using <code>algorithm: :concurrently</code> option and by making sure that the index creation will run outside of a transaction by calling <code>disable_ddl_transaction!</code>.</p>

<p>``` rb
class AddIndexToAsksActive &lt; ActiveRecord::Migration[5.0]
  disable_ddl_transaction!</p>

<p>  def change</p>

<pre><code>add_index :users, :active, algorithm: :concurrently
</code></pre>

<p>  end
end
```</p>

<p>There is one caveat here though. If you attempt to create a unique index concurrently, there is a possibility that something will go wrong, e.g., when a non-unique record is created during the index creation. Since the command is run outside the transaction, it won&rsquo;t be rolled back, and you will end up with an invalid index. Nevertheless, that is not a big problem &ndash; should it ever happen, just drop the invalid index and try creating it concurrently one more time.</p>

<h2>Wrapping up</h2>

<p>Optimizing <strong>PostgreSQL queries</strong> might not look a trivial task, but if you keep these rules in mind, you will have much easier time with your database, and you will enjoy fast queries for a long time.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rails Quick Tips: Temporarily Disabling Touching with ActiveRecord.no_touching]]></title>
    <link href="https://karolgalanciak.com/blog/2018/02/25/rails-quick-tips-temporarily-disabling-touching-with-activerecord-dot-no-touching/"/>
    <updated>2018-02-25T20:00:00+01:00</updated>
    <id>https://karolgalanciak.com/blog/2018/02/25/rails-quick-tips-temporarily-disabling-touching-with-activerecord-dot-no-touching</id>
    <content type="html"><![CDATA[<p><strong>Touching</strong> <strong>ActiveRecord models</strong> is quite a common thing in most of the <strong>Rails applications</strong>, especially useful for cache invalidation. By default, it updates <code>updated_at</code> timestamp with the current time, Here&rsquo;s a typical example of using <a href="https://apidock.com/rails/ActiveRecord/Persistence/touch">touch</a> in a model:</p>

<p>``` ruby</p>

<h1>app/models/photo.rb</h1>

<p>class Photo &lt; ApplicationRecord
  belongs_to :user, touch: true
end
```</p>

<p>Whenever a new photo is created, or the existing one is updated/destroyed, the <code>updated_at</code> attribute of the associated user will be updated with the current time. In the majority of the cases, this is the desired behavior (it&rsquo;s one of those rare ActiveRecord callbacks that is not that bad ;)). However, it might happen that you may not want <code>touch</code> to be executed for some reason. Is there any built-in solution that could solve that problem?</p>

<!--more-->


<h2>Anatomy Of The Problem</h2>

<p>Temporarily disabling <code>touch</code>ing can useful either for performance reasons (when updating a large number of records) or simply to prevent <code>after_touch</code> or <code>after_commit</code> from being executed multiple times. The latter might indicate that there is a deeper problem in the design as putting any important logic causing side-effects beyond the record&rsquo;s internal state in those <strong>ActiveRecord callbacks</strong> can easily go south (especially if you trigger email notifications), but the reality is that a lot of Rails applications use those callbacks in such cases.</p>

<h2>The Solution</h2>

<p>Fortunately, a heavy refactoring or a rewrite is not necessary. Instead, we can take advantage of <a href="http://api.rubyonrails.org/classes/ActiveRecord/NoTouching/ClassMethods.html">ActiveRecord.no_touching</a> which temporarily disables touching inside the block.</p>

<p>Imagine that you need to update all photos belonging to some user and <code>touch</code> this user only after all photos are updated. Here&rsquo;s how it could be handled:</p>

<p>``` ruby
user = User.find(user_id)</p>

<p>ActiveRecord::Base.transaction do
  User.no_touching do</p>

<pre><code>user.photos.find_each do |photo|
  # user won't be `touch`ed
  photo.update!(some_attributes)
end
</code></pre>

<p>  end</p>

<p>  user.touch
end
```</p>

<p>If for some reason disabling touching is necessary for all models, you could just call it on <code>ActiveRecord::Base</code>:</p>

<p>``` ruby
user = User.find(user_id)</p>

<p>ActiveRecord::Base.transaction do
  ActiveRecord::Base.no_touching do</p>

<pre><code>user.photos.find_each do |photo|
  # no model will be `touch`ed
  photo.update!(some_attributes)
end
</code></pre>

<p>  end</p>

<p>  user.touch
end
```</p>

<p>And that&rsquo;s it!</p>

<h2>Summary</h2>

<p><a href="http://api.rubyonrails.org/classes/ActiveRecord/NoTouching/ClassMethods.html"><code>ActiveRecord.no_touching</code></a> is certainly a quick solution to a potentially tricky issue. However, it is also a dirty hack that indicates a potential problem with the design of the application that should be addressed sooner than later.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PostgreSQL Quick Tips: Working With Dates Using EXTRACT function]]></title>
    <link href="https://karolgalanciak.com/blog/2018/01/30/postgresql-quick-tips-working-with-dates-using-extract-function/"/>
    <updated>2018-01-30T04:30:00+01:00</updated>
    <id>https://karolgalanciak.com/blog/2018/01/30/postgresql-quick-tips-working-with-dates-using-extract-function</id>
    <content type="html"><![CDATA[<p>Imagine that you are implementing an e-commerce platform and want to grab all orders from the <strong>current year</strong>. What would be the simplest way of doing it in Rails? Probably writing a query looking like this:</p>

<p><code>ruby
Order.where("created_at &gt;= ? AND created_at &lt; ?", Date.today.beginning_of_year, Date.today.beginning_of_year.next_year)
</code></p>

<p>It gets the job done but requires unnatural filtering by a range for a use case generic enough that it should be handled just using some native functions. Is it possible?</p>

<p>Apparently, it is! We can use <a href="https://www.postgresql.org/docs/10/static/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT" target="_blank"><code>EXTRACT</code></a>  and <a href="https://www.postgresql.org/docs/10/static/functions-datetime.html#FUNCTIONS-DATETIME-CURRENT" target="_blank"><code>now()</code></a> functions &ndash; the former could be used for extracting the current <strong>year</strong> from a timestamp and the latter could be used for getting the current time.</p>

<p>With those two functions, the query could look like the following one:</p>

<p><code>ruby
Order.where("EXTRACT(year FROM created_at) = EXTRACT(year FROM now())")
</code></p>

<p>Much cleaner! And the great thing is that you can also create a functional index for <code>EXTRACT(year FROM created_at)</code> to avoid sequential scanning and get much better performance.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Keeping Data Integrity In Check: Conditional Unique Indexes For Soft Delete]]></title>
    <link href="https://karolgalanciak.com/blog/2016/11/01/keeping-data-integrity-in-check-conditional-unique-indexes-for-soft-delete/"/>
    <updated>2016-11-01T00:30:00+01:00</updated>
    <id>https://karolgalanciak.com/blog/2016/11/01/keeping-data-integrity-in-check-conditional-unique-indexes-for-soft-delete</id>
    <content type="html"><![CDATA[<p><strong>Soft delete</strong> is a pretty common feature in most of the applications. It may increase complexity of the queries, nevertheless, not deleting anything might be a <strong>right default</strong> as the data might prove to be useful in the future: for restoring if a record was removed by mistake, to derive some conclusions based on statistics and plenty of other purposes. It may seem like it's a pretty trivial thing: just adding a column like <code>deleted_at</code> and filtering out records that have this value present. But what happens when you need to do some proper <strong>uniqueness validation</strong> on both model layer and database level? Let's take a look what kind of problem can easily be overlooked and how it can be solved with a <strong>conditional index</strong>.</p>




<!--more-->




<h2>Case study: daily prices for vacation rentals</h2>




<p>Let's imagine we are developing a <strong>vacation rental</strong> software. Most likely the pricing for each day will depend on some complex set of rules, but we may want to have some denormalized representation of base prices for each day to make things more obvious and have some possiblity of sharing this kind of data with other applications, which is quite common in this domain. We may start with adding a <code>DailyPrice</code> model having a reference to a <code>rental</code>, having <code>price</code> value and of course <code>date</code> for which the price is applicable.</p>




<h2>Ensuring uniqueness</h2>




<p>Obviously, we don't want to have any duplicated <code>daily_prices</code> for any rental, so we need to add a uniqueness validation for <code>rental_id</code> and <code>date</code> attributes:</p>


<p><code>ruby app/models/daily_price.rb
validates :rental_id, presence: true, uniqueness: { scope: :date }
</code></p>

<p>To ensure <strong>integrity of the data</strong> and that we are protected against race conditions and potental validation bypassing, we need to add a <strong>unique index</strong> on the database level:</p>


<p><code>ruby db/migrate/20161030120000_add_unique_index_for_daily_prices.rb
  add_index :daily_prices, [:rental_id, :date], unique: true
</code></p>

<h2>Adding soft delete functionality</h2>




<p>We have some nice setup already. But it turned out that for recalculating <code>daily_prices</code> if some rules or values influencing the price change it's much more convenient to just remove them all and recalculate from scratch than checking if the price for given date needs to be recalculated. To be on the safe side, we may decide not to hard remove these rates, but do a soft delete instead.</p>




<p>To implement this feature we could add <code>deleted_at</code> column, drop the previous index and a new one which will respect the new column. We should also update the validation in model in such case:</p>


<p><code>ruby app/models/daily_price.rb
validates :rental_id, presence: true, uniqueness: { scope: [:date, :deleted_at] }
</code></p>

<p>And the migration part:</p>


<p><code>ruby db/migrate/20161030120000_add_deleted_at_to_daily_prices.rb
  remove_index :daily_prices, [:rental_id, :date], unique: true
  add_column :daily_prices, :deleted_at, :datetime
  add_index :daily_prices, [:rental_id, :date, :deleted_at], unique: true
</code></p>

<p>Everything should be fine with that, right? What could possibly go wrong here?</p>




<h2>Adding index the right way</h2>




<p>Let's play with Rails console and check it out:</p>


<p>``` ruby</p>

<blockquote><p>time_now = Time.current
 => Sun, 23 Oct 2016 09:44:46 UTC +00:00
DailyPrice.create!(price: 100, date: Date.current, rental_id: 1, deleted_at: time_now)
  COMMIT
DailyPrice.create!(price: 100, date: Date.current, rental_id: 1)
  COMMIT
DailyPrice.create!(price: 100, date: Date.current, rental_id: 1)
  ROLLBACK
ActiveRecord::RecordInvalid: Validation failed: Rental has already been taken
```</p></blockquote>

<p>Nah, there can't be any problem, looks like we have the expected behaviour - we couldn't create a non-soft-deleted <strong>daily_price</strong> for given date and rental. But let's check one more thing:</p>


<p>``` ruby</p>

<blockquote><p>price = DailyPrice.new(price: 100, date: Date.current, rental_id: 1)
price.save(validate: false)
  COMMIT
```</p></blockquote>

<p>Whoops! Something looks very wrong here. But how is it possible? The index looks exactly like the validation in our model, yet it didn't work like that when we bypassed the validation.</p>




<p>Let's consult <strong>PostgreSQL</strong> <a href="https://www.postgresql.org/docs/9.0/static/indexes-unique.html" target="_blank">docs</a>. There is something mentioned about null values: </p>


<p><blockquote><p>Null values are not considered equal</p></blockquote></p>

<p>That is our problem: ActiveRecord considered it as a unique value, but it doesn't work quite like that in <strong>PostgreSQL</strong>.</p>




<p>Our index should be in fact <strong>conditional</strong> and look the following way:</p>


<p><code>ruby
add_index :nightly_rates, [:rental_id, :date], unique: true, where: "deleted_at IS NULL"
</code></p>

<p>We could optionally improve the validation in our model to make it look much closer to what we have in database and use <code>conditions</code> option:</p>


<p><code>ruby app/models/daily_price.rb
validates :rental_id, presence: true, uniqueness: { scope: :date, conditions: -&gt; { where(deleted_at: nil) } }
</code></p>

<p>And that's it! There's no way we could compromise <strong>data integrity</strong> now!</p>




<h2>Wrapping Up</h2>




<p>Keeping data integrity in check is essential for most of the applications to not cause some serious problems, especially when implementing soft delete. Fortunately, simply by adding PostgreSQL conditional unique indexes we can protect ourselves from such issues.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[When validation is not enough: PostgreSQL triggers for data integrity]]></title>
    <link href="https://karolgalanciak.com/blog/2016/05/06/when-validation-is-not-enough-postgresql-triggers-for-data-integrity/"/>
    <updated>2016-05-06T19:45:00+02:00</updated>
    <id>https://karolgalanciak.com/blog/2016/05/06/when-validation-is-not-enough-postgresql-triggers-for-data-integrity</id>
    <content type="html"><![CDATA[<p>Is <strong>validation</strong> in your models or form objects enough to ensure <strong>integrity of the data</strong>? Well, seems like you can't really persist a record when the data is <strong>not valid</strong> unless you intentionally try to bypass validation using <code>save: false</code> option when calling <code>save</code> or using <code>update_column</code>. What about <strong>uniqueness validation</strong>?
A classic example would be a unique <code>email</code> per user. To make sure the email is truly unique we could add a unique index in database - not only would it prevent saving non-unique users when <strong>bypassing validation</strong>, but also it would raise extra error when 2 <strong>concurrent requests</strong> would attempt to save user with the same email. However, some validations are more complex that ensuring  a value is unique and index won't really help much. Fortunately, PostgreSQL is powerful enough to provide a perfect solution to such problem. Time to meet your new friend: <strong>PostgreSQL triggers</strong>.</p>




<!--more-->




<h2>Anatomy of PostgreSQL triggers and procedures</h2>




<p>PostgreSQL trigger is like a callback: it's a function that is called on specific event: <code>before</code> or <code>after</code> <code>insert</code>, <code>update</code>, <code>delete</code> or <code>truncate</code> in case of tables and views and for views you can also run a function <code>instead of</code> those events. Triggers can be run either <code>for each row</code> (tables only) and <code>for each statement</code> (both tables and views). The difference between them is quite simple: <code>for each row</code> is run for every modified row and <code>for each statement</code> is run only once per statement. The important thing to keep in mind regarding <code>for each row</code> is that you have a reference to the row being modified, which will be essential in upcoming example.</p>




<p>By running <code>\h CREATE TRIGGER;</code> from <strong>psql</strong> we can get a generalized syntax for creating triggers:</p>


<p>```
Command:     CREATE TRIGGER
Description: define a new trigger
Syntax:
CREATE [ CONSTRAINT ] TRIGGER name { BEFORE | AFTER | INSTEAD OF } { event [ OR &hellip; ] }</p>

<pre><code>ON table_name
[ FROM referenced_table_name ]
[ NOT DEFERRABLE | [ DEFERRABLE ] { INITIALLY IMMEDIATE | INITIALLY DEFERRED } ]
[ FOR [ EACH ] { ROW | STATEMENT } ]
[ WHEN ( condition ) ]
EXECUTE PROCEDURE function_name ( arguments )
</code></pre>

<p>where event can be one of:</p>

<pre><code>INSERT
UPDATE [ OF column_name [, ... ] ]
DELETE
TRUNCATE
</code></pre>

<p>```</p>

<p>You can also add a <b>condition</b> for running <strong>triggers</strong> and timing option, which I'm not going to discuss in greater details, you can find more about them in official <a href="http://www.postgresql.org/docs/9.5/static/sql-createtrigger.html" target="_blank">docs</a>.</p>




<p>What about <code>function_name</code>? It's a user-defined function returning <code>trigger</code>. Here's a dummy example to give an idea about the syntax for defining functions:</p>


<p>```
CREATE FUNCTION dummy() RETURNS trigger AS $$
DECLARE</p>

<pre><code>some_integer_variable int;
</code></pre>

<p>BEGIN</p>

<pre><code>some_integer_variable := 1;
NEW.counter := some_integer_variable;
RETURN NEW;
</code></pre>

<p>END;
$$ language plpgsql;
```</p>

<p>We started with defining <code>dummy</code> function taking no arguments returning type of <strong>trigger</strong>. Next, we have a <code>DECLARE</code> block where we declare temporary variables, in our case it's <code>some_integer_variable</code> of type <code>int</code>. Within <code>BEGIN / END</code> block we define the actual function body: we assign a dummy value to <code>some_integer_variable</code> variable using <code>:=</code> operator and then we do some assignment using implicit <code>NEW</code> variable which is basically a row referenced by given statement. This variable is available only when running a trigger <code>for each row</code>, otherwise it will return <code>NULL</code>. Any trigger has to return either a <strong>row</strong> or <code>NULL</code> - in this example we return <code>NEW</code> row. At the end we have a declaration of writing function in <code>plpgsql</code> language.</p>




<p>Let's take a look at some real world code to see triggers in action.</p>




<h2>Using triggers for data integrity</h2>




<p>A good example where we could use a trigger for more complex validation could be a calendar event: we need to ensure that no other event exists between some <code>begins_at</code> time and <code>finishes_at</code> time. We should also scope it only to a given calendar and exclude id of event being updated - when creating new events it wouldn't matter, but without excluding id we wouldn't be able to update any event. So what we actually want to achieve is to create a trigger that will be run <code>before insert or update</code> on <b>events</b> table <code>for each row</code>.</p>




<p>Let's start with generating <code>Calendar</code> model and <code>Event</code> model with reference to <code>calendar</code> and <code>begins_at</code> and <code>finishes_at</code> attributes:</p>


<p><code>
rails generate model Calendar
rails generate model Event calendar_id:integer begins_at:datetime finishes_at:datetime
</code></p>

<p>and also generate extra migration adding <strong>trigger</strong> and <strong>procedure</strong> ensuring that only one event can be created for given period of time for a calendar:</p>


<p><code>
rails generate migration add_validation_trigger_for_events_availability
</code></p>

<p>add the SQL code :</p>


<p>``` ruby
class AddValidationTriggerForEventsAvailability &lt; ActiveRecord::Migration
  def change</p>

<pre><code>execute &lt;&lt;-CODE
  CREATE FUNCTION validate_event_availability() returns trigger as $$
  DECLARE
    events_count int;
  BEGIN
    events_count := (SELECT COUNT(*) FROM events WHERE (
      events.calendar_id = NEW.calendar_id AND events.begins_at &lt; NEW.finishes_at AND events.finishes_at &gt; NEW.begins_at AND events.id != NEW.id
    ));
    IF (events_count != 0) THEN
      RAISE EXCEPTION 'Period between % and % is already taken', NEW.begins_at, NEW.finishes_at;
    END IF;
    RETURN NEW;
  END;
  $$ language plpgsql;

  CREATE TRIGGER validate_event_availability_trigger BEFORE INSERT OR UPDATE ON events
  FOR EACH ROW EXECUTE PROCEDURE validate_event_availability();
CODE
</code></pre>

<p>  end
end
```</p>

<p>Our <code>validate_event_availability</code> function performs query to count all events that are between given time period for specified calendar excluding own id (so that the row being updated is not considered here, which would prevent updating any event). If any other event is found, the exception is raised with an error message - <code>%</code> characters are used for interpolation of <code>begins_at</code> and <code>finishes_at</code> attributes. If no other event is found, we simply return the row.</p>




<p>We want to define a trigger running this function before creating any new event or updating existing ones, so we need to run it <code>BEFORE INSERT OR UPDATE</code> <code>FOR EACH ROW</code>.</p>




<p>It might be a good idea to switch also to <code>:sql</code> schema format - the standard <code>:ruby</code> format can't handle triggers at this point. Add this line in <code>config/application.rb</code>:</p>


<p>``` ruby</p>

<h1>config/application.rb</h1>

<p>config.active_record.schema_format = :sql
```</p>

<p>Now we can run migrations:</p>


<p><code>
rake db:migrate
</code></p>

<p>After changing the schema format, new <code>structure.sql</code> file should be created. It's not going to look that nice like <code>schema.rb</code>, but at least it contains all the details. Let's try creating some events from <code>rails console</code>:</p>


<p><code>ruby
calendar = Calendar.create
begins_at = "2016-05-02 12:00:00"
finishes_at =  "2016-05-02 16:00:00"
Event.create(calendar_id: calendar.id, begins_at: begins_at, finishes_at: finishes_at)
</code></p>

<p>Creating first event obviously works, but what's going to happen when we try to create an event with exactly the same dates?</p>


<p>``` ruby
Event.create(calendar_id: calendar.id, begins_at: begins_at, finishes_at: finishes_at)</p>

<p>ActiveRecord::StatementInvalid: PG::RaiseException: ERROR:  Period between 2016-05-02 12:00:00 and 2016-05-02 16:00:00 is already taken
```</p>

<p>Awesome (haha, doesn't happen that often to be happy when an error occurs ;)), that's exactly what we wanted to achieve - the trigger keeps our data safe making sure that we won't have any duplicated events or events covering the same time period. The last thing we should do is to mimic such validation and add it to form object or model for better user experience, but it's beyond the scope of this article. It's going to duplicate some logic between the code in the application and the database, but in this case there's no way to <strong>DRY</strong> it up.</p>




<h2>Wrapping up</h2>




<p>PostgreSQL triggers and procedures are not something that you will often want to use in Rails applications, but sometimes there's no other solution, especially when you have more complex rules for data integrity. In such cases, triggers and procedures are the right tool for the job.</p>



]]></content>
  </entry>
  
</feed>
