<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Rails | Karol Galanciak - Ruby on Rails and Ember.js consultant]]></title>
  <link href="https://karolgalanciak.com/blog/categories/rails/atom.xml" rel="self"/>
  <link href="https://karolgalanciak.com/"/>
  <updated>2019-02-24T22:10:12+01:00</updated>
  <id>https://karolgalanciak.com/</id>
  <author>
    <name><![CDATA[Karol Galanciak]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Messages on Rails Part 1 -  Introduction to Kafka and RabbitMQ]]></title>
    <link href="https://karolgalanciak.com/blog/2019/02/24/messages-on-rails-part-1-introduction-to-kafka-and-rabbitmq/"/>
    <updated>2019-02-24T22:00:00+01:00</updated>
    <id>https://karolgalanciak.com/blog/2019/02/24/messages-on-rails-part-1-introduction-to-kafka-and-rabbitmq</id>
    <content type="html"><![CDATA[<p><strong>Microservices</strong>, <strong>Service-Oriented Architecture (SOA)</strong> and in general, <strong>distributed ecosystems</strong>, have been on hype in the last several years. And that&rsquo;s for a good reason! At certain point, The <strong>Majestic Monolith</strong> &ldquo;pattern&rdquo; might start causing issues, both from the purely technical reasons like scalability, tight coupling of the code if you don&rsquo;t follow <strong>Domain-Driven Design</strong> or some other practices <strong>improving modularity</strong>, maintenance overhead, and also from organizational perspective since working in smaller teams on smaller apps is more efficient than working with huge team on an even bigger monolith which suffers from tight coupling and low cohesion. However, this is only true if the overall architecture addresses the potential problems that are common in the micro/macro-services world. One of these problems I would like to focus on is communication between apps and how the data flows between them.</p>

<!--more-->


<h2>How Can The Applications Talk To Each Other</h2>

<p>There are multiple ways the applications could talk to each other but let&rsquo;s focus on the most popular ones: shared database, HTTP API and messages.</p>

<h3>Shared Database</h3>

<p>It is arguable if we can talk about apps communicating with each other in this scenario; nevertheless, it&rsquo;s one of the ways to solve the problem of getting data from one application to another. And apart from some particular scenarios, it&rsquo;s the way that could give you a lot of headaches quite soon.</p>

<p>Initially, it might look like a great idea &ndash; you connect the other app to an existing database, and you don&rsquo;t need to worry about the communication! If the other application is just responsible for reading from that database, maybe you won&rsquo;t have any severe problems beyond increased load, issues of scaling and questionable access control (by default, the other app can do anything to that database). However, if you have multiple applications writing to the same database, you will undoubtedly have more serious issues &ndash; deadlocks, problems with data consistency as different constraints might apply to different applications, schema getting out of control with each application adding some custom tables and fields, which can turn out to be quite nasty &ndash; imagine one application acquiring Access Exclusive Lock on the entire table which turns out to be large (so the lock will not be released any time soon) thanks to some migration, while the other app was supposed to do some business-critical operations on that table that were also time-sensitive. Some of these problems are RDBMS-specific like locking during migration, but there is a very good chance anyway that you will be using either MySQL or PostgreSQL.</p>

<p>There are just too many things that can go wrong with having shared database that I would highly discourage from even trying that approach, maybe besides one single scenario &ndash; when what you need is exactly a replication of some data, e.g., to generate reports. In that case, you can set up PostgreSQL logical replication and let it replicate the data from the primary data source to a read-only replica containing only the data the application needs. Since PostgreSQL has a very reliable replication mechanism, you don&rsquo;t need to reinvent the wheel, and you can take advantage of that feature. Keep in mind that logical replication was introduced in PostgreSQL 10, so it&rsquo;s not available in the previous versions.</p>

<h3>HTTP API</h3>

<p>HTTP APIs, especially the RESTful APIs (although GraphQL has been getting quite popular in the last years), are arguably the most common blocks in microservices architecture as far as the communication between applications goes. REST architecture goes very well with HTTP, the communication between client and server is synchronous which makes it easy to reason about and debug, HTTP APIs are ubiquitous, and the developers are familiar with working with them. Also, HATEOAS and capabilities of getting precisely what you need from GraphQL make using with HTTP APIs easy, especially when working with Third-Party applications. Also, there is a lot of flexibility in implementing access control and authentication. On top of that, there are <a href="https://jsonapi.org">specifications</a> that clearly define how JSON APIs should be built, which minimizes reinventing the wheel and makes it even easier to build a client for that kind of API.</p>

<p>Based on those advantages it might look like communication between services via JSON APIs is the best possible solution to the problem, right?</p>

<p>Well, not necessarily. If you have one or two applications communicating with some other application, maybe it&rsquo;s not a bad idea, especially if the traffic is not huge and you don&rsquo;t need to transfer gigabytes of JSON daily via HTTP.</p>

<p>However, what if you expect a massive load from the services and indeed you will be transferring gigabytes of JSON via HTTP? Do you really need flexible authorization for communication between internal services? And why deal with an extra overhead of non-persistent connections and SSL handshakes in the first place?</p>

<p>Those things are great for integration with Third-Party applications, but they bring very little value in a closed ecosystem. Nevertheless, that overhead is not the biggest problem here, let&rsquo;s discuss something way more problematic: scaling.</p>

<p>Imagine that the API of one application is exposed to multiple other applications in your ecosystem. Also, the same application is used by &ldquo;real&rdquo; users who interact with it via UI. The immediate problem we have with that approach is that the traffic generated by the other services will be affecting regular users, which makes scaling decisions more tricky. To make it more complicated, let&rsquo;s say that in our example ecosystem we have a lot of data, and each application will be reading gigabytes of data every day and each service fetches data periodically from the API, e.g., every 5 minutes. To limit the number of requests so that we don&rsquo;t fetch the data that has been already fetched by the applications, we may want to implement some sort of data offset, which most likely will be based on the timestamp of the last fetch. We could send this timestamp that will filter out the records that haven&rsquo;t changed after that time and drastically reduce the number of requests that way. However, in a big enough scale, that will not be enough to solve all the issues. There can be thousands of records to be fetched every 5 minutes, which will most likely be paginated &ndash; that means a lot of requests. Multiply it by the number of applications and the traffic looks even more depressing, especially if every application will fetch exactly the same data.</p>

<p>Either the answer to the problem will be caching, which might be tricky to implement since the services will be using timestamp-based offset, or we can switch from pull-based strategy to push-based strategy.</p>

<p>A push-based strategy will be a publish-subscribe pattern via HTTP. The benefit of that is that we could serialize the state of the record after every change only once (or just serialize a changeset and go in the direction of CQRS/Event Sourcing) and send the payload to every service subscribed to a given event. The benefit of this approach is serializing a payload only once instead of doing it on every request, we can avoid useless requests which might happen if nothing has changed in the last 5 minutes and we could balance the load efficiently by moving the delivery logic to some background jobs. The disadvantage of publish-subscribe via HTTP, a.k.a. webhooks, would be implementing HTTP APIs in every service. Also, each application will experience massive traffic on a large enough scale.</p>

<p>It&rsquo;s great if your ecosystem offers this kind of features for limiting the number of requests to Third-Party Applications, but is it the most efficient way of handling communication between internal applications and scaling the ecosystem?</p>

<p>This type of communication is arguably quite common in the microservices world, and the funny thing is that in some aspects it&rsquo;s no different than reimplementing messaging systems, but done in a less effective way.</p>

<p>There is a more efficient way to solve deal with the communication between internal services: using asynchronous messaging.</p>

<h3>Asynchronous Messaging</h3>

<p>Async messaging is a communication approach where a middleware, called message broker, connects producers of messages with consumers of those messages. Usually, a producer sends messages and the message broker is responsible for delivering them to proper consumers.</p>

<p>Thanks to having a message broker in-between the services, not only do we benefit from extra decoupling between the applications, but also we have way more flexibility as far as scalability goes.</p>

<p>Let&rsquo;s take a look at two very popular message brokers that take different approaches to messages: <a href="https://www.rabbitmq.com">RabbitMQ</a> and <a href="http://kafka.apache.org">Apache Kafka</a></p>

<h4>RabbitMQ</h4>

<p>RabbitMQ is a general purpose message broker supporting multiple protocols from which AMQP will be the one most interesting one to us. It implements a smart broker/dumb consumer model, which means the broker is responsible for delivering messages to the right place.</p>

<p>The essential design decision in RabbitMQ is that the producers shouldn&rsquo;t directly push messages to the queues, from which the consumers can read messages and process them. Instead, producers send messages to exchanges, and the queues are connected to these exchanges. That way, the producer doesn&rsquo;t know where exactly the message will be delivered and how many consumers are going to do anything with it if any! This kind of decoupling not only allows implementation of the publish-subscribe pattern, where multiple consumers using different queues bind to the same exchange and process the same message, but it also opens the way to quite a flexible routing (which we will cover in more details in the next part of this series). Once the consumer acknowledges the message, it is removed from the queue. Multiple producers can also subscribe to the same queue which will make them compete for the messages. The communication in a simple scenario as described above is illustrated in the following diagram:</p>

<p class="img-center-wrapper">
  <img class="img-center" src="/images/messages_on_rails_part_1/rabbitmq.png" title="RabbitMQ" alt="RabbitMQ">
</p>




<p class="center">
  <a href="https://karolgalanciak.com/images/messages_on_rails_part_1/rabbitmq.png" target="_blank">See in better quality</a>
</p>


<p>RabbitMQ is an excellent choice if you have complex routing scenarios, priority queues, you don&rsquo;t care about keeping messages in the queue after they are processed, and you don&rsquo;t expect an extreme throughput (although 100k messages/s that you can get in some <a href="https://www.cloudamqp.com/plans.html">RabbitMQ As A Service solutions</a>, even in a high availability scenario, is in most cases more than enough).</p>

<h4>Kafka</h4>

<p>Kafka is a distributed streaming platform which takes a different approach than RabbitMQ &ndash; it implements a dumb broker/smart consumer model. In that case,  it is the responsibility of the consumers to fetch messages and keep track of their state. What is unique about Kafka is the fact that it is not a message queue &ndash; it is an append-only log. Producers send messages to the topics, to which consumers subscribe and read the messages and keep the offset which is the information about the position in the log until which the messages were read. It can be illustrated the following way:</p>

<p><img src="/images/messages_on_rails_part_1/kafka.png" title="&lsquo;Kafka&rsquo; &lsquo;Kafka&rsquo;" ></p>

<p class="center">
  <a href="https://karolgalanciak.com/images/messages_on_rails_part_1/kafka.png" target="_blank">See in better quality</a>
</p>


<p>Such a design opens a way to some interesting features. The log is persistent with configurable retention which can be based on the size or time how long the events should be kept in the log, which makes Kafka act like a circular buffer. To save some space, Kafka allows log compaction when the messages are sent with the same key, keeping only the last one as a result. If storage is not an issue, you might even decide never to never delete any messages!</p>

<p>On top of that, Kafka can easily handle a massive throughput &ndash; 100 000 messages/second is not extraordinary, it&rsquo;s easy to scale, and there are <a href="https://events.static.linuxfound.org/sites/events/files/slides/Kafka%20At%20Scale.pdf">cases</a> of handling millions of messages per second.</p>

<p>A persistent log is a killer-feature of Kafka. When you add another service to your ecosystem and need to get the data from one app to another, you will run into obvious issues when using a traditional message queue since the messages are gone after they are processed. With Kafka, just can add a new consumer and replay all the past events! You can also combine Kafka with stream processing frameworks like <a href="http://samza.apache.org">Apache Samza</a>, <a href="http://spark.apache.org">Apache Spark</a>, <a href="http://storm.apache.org">Apache Storm</a>,<a href="https://www.confluent.io/product/ksql/">KSQL</a> or use Kafka Streams.</p>

<p>Kafka is an excellent choice for use cases where the routing is not complex, the throughput is substantial, the messages need to stay in the log (even indefinitely) and where you expect the need to replay messages. You can also do Event Sourcing with Kafka and use stream processor like Kafka Stream or KSQL to construct projections from the events.</p>

<h2>Summary</h2>

<p><strong>Communication</strong> between multiple services in a <strong>distributed ecosystem</strong> is far from a trivial problem, which can be approached in various ways: via a <strong>shared database</strong> (although to a very limited extent), <strong>HTTP APIs</strong> or <strong>messages</strong>.</p>

<p>In the next parts I&rsquo;ll be focusing on <strong>Kafka</strong> and <strong>RabbitMQ</strong> &ndash; how they work and how to use them in Rails applications, how to produce and consume messages, what kind of extra options you get by introducing Kafka and RabbitMQ to your ecosystem, potential issues you might have in some specific scenarios and show some examples where combining both could bring benefits.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Problems With Validating ActiveRecord Models And Why State Validation Is a Bad Idea]]></title>
    <link href="https://karolgalanciak.com/blog/2018/09/30/the-problems-with-validating-activerecord-models-and-why-state-validation-is-a-bad-idea/"/>
    <updated>2018-09-30T20:00:00+02:00</updated>
    <id>https://karolgalanciak.com/blog/2018/09/30/the-problems-with-validating-activerecord-models-and-why-state-validation-is-a-bad-idea</id>
    <content type="html"><![CDATA[<p>In the typical <strong>Rails application</strong>, you can find the most of the validations in the <strong>ActiveRecord models</strong>, which is nothing surprising &ndash; ActiveRecord models are used for multiple things. Whether it is a good thing, or a bad thing (in most cases it&rsquo;s the latter) deserves a separate book or at least blog post-series as it&rsquo;s not a simple problem, there is one specific thing that can cause <strong>a lot of issues</strong> that are <strong>difficult to solve</strong> and go beyond <strong>design decisions</strong> and ease of maintenance of the application, something that impacts the behavior of the model &ndash; <strong>the validations</strong>.</p>

<p>Just to give you a real-world example of what validation in ActiveRecord model looks like (as impossible as it seems, it really did happen) &ndash; when updating the check-in time of the reservation, which is a simple attribute on Reservation model, the record turned out to be invalid because&hellip; the format of guest&rsquo;s phone didn&rsquo;t match some regexp.</p>

<p>There are multiple ways to bypass this problem: use <code>validate: false</code> flag with <code>save</code> method: <code>save(validate: false)</code> or use <code>update_columns</code> method, but this is definitely not something that can be applied in a &ldquo;normal&rdquo; use case. In a typical scenario, this will be the error message displayed in the UI/returned to API consumer, and it will be confusing.</p>

<p>However, this is the expected behavior of ActiveRecord (or in general, ActiveModel-style) validations, which is <strong>a validation of the state</strong> of the model. And judging from this example, it&rsquo;s evident that it leads to <strong>problematic scenarios</strong>. What kind of design then would be the most appropriate to <strong>prevent such issues</strong>?</p>

<!--more-->


<h2>Forget State Validation</h2>

<p>Based on the previous example, it&rsquo;s clear that the real problem is the idea of a model&rsquo;s state validation. And the more complex state of the models can be (especially if there are some cross-validations between several models, which is not uncommon in complex applications), the more problems you will get.</p>

<p>Just wanted to update some <code>notes</code> attribute to add some quick info about something? Forget it &ndash; you will get three different validation errors that will tell you that someone&rsquo;s email has an invalid format, description is too short, a discount amount of something is invalid.</p>

<p>Sadly, this is the typical Rails Way of handling validations. In the initial phase of the application, this is certainly convenient &ndash; adding new validations is super easy, there is no need to discuss the design of potential alternatives and how it fits the bigger picture, and there is very little overhead. Unless you are ok with such problems, at some point, you will probably need to migrate validations to some other solution. What would be the potential alternative for validations?</p>

<h2>Validate The Actual Use Case</h2>

<p>The answer to that question is straight-forward (although it doesn&rsquo;t mean that achieving it will be simple) &ndash; just validate the thing you are doing, the actual use case. If you want to update <code>description</code>, you should only be concerned with the description &ndash; if it&rsquo;s present or not, it&rsquo;s length, etc., whether <code>headline</code> is too short or not (this can happen when you change the validation rules and don&rsquo;t somehow migrate the data) is not the subject of that use case.</p>

<p>There are a lot of implications of such approach &ndash; indeed, it will result in different sets of validators per creation and per update, since for creation we usually need way more data, and for an update, we may merely want to update a single attribute. Effectively, it will result in different validation pipelines for create and update actions. For creating, we may always need to apply specific validators (e.g., presence validators for some attributes), but for updating it will make more sense to apply only the validators for what we are trying to do &ndash; when we want to update email, we apply the presence and format validation for an email, if we want to update a description, we apply presence and length validators for a description.</p>

<h2>Potential Implementation</h2>

<p>What would be the way to implement it? The specific design is out of the scope of this article as it might require building mini-framework for validations and consider some design implications on the entire application (especially if we are escaping the traditional Rails Way). However, one thing is sure here &ndash; there will be dedicated validator objects, probably different for create and update action, and the validations will need to be removed from ActiveRecord models.</p>

<p>A potential way of interacting with such objects could look like this:</p>

<p><code>rb
validation_result = Reservation::CreateValidator.call(params)
if validation_result.success?
  # handle happy path here
else
  puts validation_result.errors.messages # for convenience and familiarity, `errors` object could have a similar interface to ActiveModel::Errors
end
</code></p>

<p>What if we need the model itself in the validator due to some complex business rules, like cross-model validation? We could either reuse <code>id</code> from <code>params</code> or just provide <code>model</code> as an argument:</p>

<p><code>rb
validation_result = Reservation::CreateValidator.call(record, params)
</code></p>

<h2>Side-Effects Of Such Design</h2>

<p>The implications of such design go far deeper than just moving things from one place to another to prevent some edge cases (and naturally increasing the complexity of the design, but it seems to be a fair price to pay for what we get as a result). Since it would make the most sense to have validations per use case, then&hellip; maybe we can have use case as objects that would expose their constraints, and the validators would take the rules from those objects and apply some specific logic on top of it to achieve the desired result? Maybe we could even create value objects composed of a single or multiple attributes, e.g. <code>Client::Email</code> object that would enforce its constraints and also, move some logic specific to the email itself in the context of a hypothetical <code>Client</code> model? And if we can identify the use cases themselves, aren&rsquo;t they domain events? And how hard would it be to build event-based architecture, or even apply Event Sourcing?</p>

<p>These are not trivial questions, however, doing one change in the design opens
the door to a holistic architectural approach where all parts of the domain fit together and the interaction between them is way more intentional comparing to ad hoc duct-tape-like solutions.</p>

<h2>Wrapping Up</h2>

<p>Putting <strong>too much logic</strong> in <strong>ActiveRecord models</strong> has a lot of disadvantages, most of them being problematic on the <strong>design level</strong>. However, things like validations can result in some nasty issues that go beyond the maintenance and cause actual business problems. Fortunately, by keeping that in mind and putting the <strong>validation logic</strong> in a <strong>separate object(s)</strong>, we can easily avoid such issues and as a nice side-effect, have a design that is way more flexible, extendible and eventually simpler to maintain.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Indexes on Rails: How to make the most of your Postgres database]]></title>
    <link href="https://karolgalanciak.com/blog/2018/08/19/indexes-on-rails-how-to-make-the-most-of-your-postgres-database/"/>
    <updated>2018-08-19T20:00:00+02:00</updated>
    <id>https://karolgalanciak.com/blog/2018/08/19/indexes-on-rails-how-to-make-the-most-of-your-postgres-database</id>
    <content type="html"><![CDATA[<p>Optimizing <strong>database queries</strong> is arguably one of the fastest ways to improve the <strong>performance</strong> of the Rails applications. There are multiple ways how you can approach it, depending on the kind of a problem. <strong>N+1 queries</strong> seem to be a pretty common issue, which is, fortunately, <strong>easy to address</strong>. However, sometimes you have some relatively <strong>simple-looking queries</strong> that seem to take way longer than they should be, indicating that they might require some optimization. The best way to improve such queries is adding a <strong>proper index</strong>.</p>

<p>But what does &ldquo;proper index&rdquo; mean? How to figure out what kind of index is exactly needed for a given query? Here are some essential facts and tips that should cover a majority of the queries you may encounter and make your database no longer a bottleneck.</p>

<!--more-->


<h2>Why index at all?</h2>

<p>Simple &ndash; to have faster queries. But why are indexes faster? The alternative to index is a sequential scanning of the entire table. That might not sound like a bad idea, but imagine you are performing a search over a huge table. What would be the fastest way to retrieve all records you are looking for &ndash; by scanning the entire table, or maybe having a way to store a subset of the records, based on some specific criteria, and then retrieve them from that place? Obviously, it&rsquo;s the second option. And that&rsquo;s roughly how indexes work.</p>

<p>As trivial as it sounds, there is a valuable lesson to learn from it: to achieve a good performance, the index must be selective enough. And the more specific you will be about those criteria, the better.</p>

<h2>Index Types</h2>

<p>Although Postgres by defaults creates <code>B-Tree</code> index when using <code>CREATE INDEX</code> command, there are a couple of more indexes that will be certainly useful in many use cases. Let&rsquo;s check them all out:</p>

<h3>B-Tree Index</h3>

<p><code>B-Tree</code> is a self-balancing tree data structure which keeps data ordered and easy to search. This index is appropriate for equality and range queries (using operators like <code>&gt;=</code>,  <code>&lt;</code> etc.) and will work great with text, timestamp and number fields.</p>

<p>B-Tree indexes are a reasonable default for most of the queries, but not for all of them. The limitation comes from the underlying structure. Discussing the details of the B-Tree data structure itself is beyond the scope of this article; nevertheless, it&rsquo;s worth keeping in mind that it&rsquo;s a similar data structure to a binary search tree, which has meaningful consequences on what can be indexed with it and how. We will get back to a couple of examples later.</p>

<h3>Hash Index</h3>

<p>Before Postgres 10, the usage of hash indexes was discouraged since they used to be not WAL-logged. Fortunately, it&rsquo;s changed in Postgres 10, and we can use them safely without worrying about rebuilding the index if something goes wrong with our database that would cause a crash. The use cases where hash indexes are useful are very limited, as they work only for equality, but they are a bit more efficient for this kind of queries comparing to b-tree indexes. If you store tokens for example and perform lookups by the token value, hash indexes would be a good way to optimize such queries.</p>

<h3>BRIN Index (Block Range Index)</h3>

<p>BRIN indexes were introduced in Postgres 9.5 which make them a pretty new addition. They tend to work very well for the large sets of ordered data, e.g., statistical data collected with timestamps which are later filtered by the time range. They will perform better than b-tree indexes in such case, although the difference won&rsquo;t be drastic. However, the different of the size of the index will be huge &ndash; BRIN index can be smaller by literally few orders of magnitude comparing to b-tree index.</p>

<h3>GIN Index (Generalized Inverted Index)</h3>

<p>GIN Indexes are the perfect choice for &ldquo;composite values&rdquo; where you perform a query which looks for an element within such &ldquo;composite&rdquo;. That is the index you will most likely want to use for <code>jsonb</code>, <code>array</code> or <code>hstore</code> data structures. They are also an excellent choice for full-text search.</p>

<h3>GiST Index (Generalized Inverted Seach Tree Index)</h3>

<p>GiST Indexes will be a good choice when the records overlap values under the same column. They are commonly used for geometry types and full-text search as well. The difference between GIN and GiST Index when it comes to full-text search is that GiST Index will be less taxing on writes comparing to GIN (as it is faster to build). But since it&rsquo;s a lossy index, there might be some extra overhead involved for reads, which makes GIN index a better choice when you mostly care about reads optimization.</p>

<h2>Optimizing Queries</h2>

<p>Here are some tips that should help you with the majority of the queries:</p>

<h3>Start with EXPLAIN</h3>

<p>With enough experience and knowledge of your app, you will develop an intuition about indexes and where they might be useful, long before having performance problems with queries. Until that happens, it&rsquo;s essential to understand how Postgres is going to execute these queries. The best tool for that purpose is using <code>EXPLAIN</code> command, which will show the execution plan generated by the query planner. <code>ActiveRecord</code> provides a convenient method &ndash; <code>explain</code> &ndash; that you can use on collections to get the query plan:</p>

<p>```</p>

<blockquote><p>Order.where(customer_id: 1).explain
  Order Load (13.8ms)  SELECT &ldquo;orders&rdquo;.<em> FROM &ldquo;orders&rdquo; WHERE &ldquo;orders&rdquo;.&ldquo;customer_id&rdquo; = $1  [[&ldquo;customer_id&rdquo;, 1]]
=> EXPLAIN for: SELECT &ldquo;orders&rdquo;.</em> FROM &ldquo;orders&rdquo; WHERE &ldquo;orders&rdquo;.&ldquo;customer_id&rdquo; = $1 [[&ldquo;customer_id&rdquo;, 1]]</p>

<pre><code>                                       QUERY PLAN
</code></pre>

<hr />

<p> Index Scan using index_orders_on_customer_id on orders  (cost=0.15..19.62 rows=50 width=1417)
   Index Cond: (customer_id = 1)
(2 rows)
```</p></blockquote>

<h3>How to tell a good query plan from a bad one?</h3>

<p>This is not that simple as it sounds, as the sequential scan can be sometimes more efficient than using an index, especially if not kept in memory, but stored entirely on disk, and even worse, an HDD one. Usually, a preferable query plan is the one that looks simpler and utilizes the least possible number of indexes, which means that it&rsquo;s better to use one index instead of two of them, like in the following example:</p>

<p>```</p>

<blockquote><p>Product.where(warehouse_id: 1).where(category_id: 1).explain
  Product Load (0.5ms)  SELECT &ldquo;products&rdquo;.<em> FROM &ldquo;products&rdquo; WHERE &ldquo;products&rdquo;.&ldquo;warehouse_id&rdquo; = $1 AND &ldquo;products&rdquo;.&ldquo;category_id&rdquo; = $2  [[&ldquo;warehouse_id&rdquo;, 1], [&ldquo;category_id&rdquo;, 1]]
=> EXPLAIN for: SELECT &ldquo;products&rdquo;.</em> FROM &ldquo;products&rdquo; WHERE &ldquo;products&rdquo;.&ldquo;warehouse_id&rdquo; = $1 AND &ldquo;products&rdquo;.&ldquo;category_id&rdquo; = $2 [[&ldquo;warehouse_id&rdquo;, 1], [&ldquo;category_id&rdquo;, 1]]</p>

<pre><code>                                                   QUERY PLAN
</code></pre>

<hr />

<p> Bitmap Heap Scan on products  (cost=9.08..13.10 rows=1 width=1417)
   Recheck Cond: ((warehouse_id = 1) AND (category_id = 1))
   &ndash;>  BitmapAnd  (cost=9.08..9.08 rows=1 width=0)</p>

<pre><code>     -&gt;  Bitmap Index Scan on index_products_on_warehouse_id_and_name_and_something_else  (cost=0.00..4.31 rows=5 width=0)
           Index Cond: (warehouse_id = 1)
     -&gt;  Bitmap Index Scan on index_products_on_category_id  (cost=0.00..4.52 rows=50 width=0)
           Index Cond: (category_id = 1)
</code></pre>

<p>(7 rows)
```</p></blockquote>

<p>It doesn&rsquo;t necessarily mean that this query plan is a bad one &ndash; it could be totally the case that such query is fast enough. However, if read speed is more important for us than the index size and extra overhead on writes which will make them slower, the best way to deal with such query would be adding a compound index on both <code>warehouse_id</code> and <code>category_id</code>.</p>

<p>One statement that is especially worth keeping an eye on (besides <code>Seq Scan</code> which stands for a sequential scan) is <code>Filter</code> statement which indicates that the records required extra filtering and the index was not enough. Here is one example:</p>

<p><code>
 Index Scan using index_products_on_category_id on products
   Index Cond: (category = 1)
   Filter: (created_at = '2018-08-11'::date)
</code></p>

<p>Ideally, <code>created_at</code> part would appear in <code>Index Cond</code> and be fully covered by the index. Usually, adding a compound index on multiple columns solves the issue which in this example would mean having an index on both <code>category_id</code> and <code>created_at</code>, not only on <code>category_id</code>.</p>

<h3>Sequence of the columns in B-Tree index does matter</h3>

<p>The sequence of the columns in a multi-column index is critical. Imagine that you created a following index: <code>create_index :tag_items, [:taggable_type, :taggable_id]</code> and want to perform a couple of queries. For sure this index is going to be efficient for searching by both <code>taggable_type</code> and <code>taggable_id</code>. It will also work great for search by <code>taggable_type</code>. It won&rsquo;t, however, be efficient when performing a search just by <code>taggable_id</code>. The reason why it is like that is quite simple though &ndash; try to imagine how the data would be stored in a hypothetical B-Tree. First, the nodes will be organized based on the leftmost column and then, by another one. Traversing such tree when you do a search based on <code>taggable_type</code> or both <code>taggable_type</code> and <code>taggable_id</code> will be simple. However, you can&rsquo;t do the same with just <code>taggable_id</code>. Postgres might use this index anyway as it might turn out to be still more efficient than a sequential scan, but this is going to be suboptimal. If it happens that you need to perform queries by <code>taggable_id</code> only, it would be a good idea to add a separate index on that field.</p>

<h3>Unique Indexes</h3>

<p>The biggest need behind unique indexes is ensuring data integrity (since most uniqueness validations, including ActiveRecord one, don&rsquo;t enforce anything and are more useful for having a nice error message and not raising an exception than for data integrity). However, a nice side effect of a unique index is also a better performance comparing to a non-unique one.</p>

<h3>Partial Indexes</h3>

<p>Imagine that you have some Articles in your application and you want to add <code>published_at</code> datetime field indicating whether and when the article was published, and then, filter published articles by a given author. We can most likely expect a need for an index on <code>author_id</code> column in such case. What about our second condition? We could for sure add a compound index on both <code>author_id</code> and <code>published_at</code>. However, there is a better choice. We could add a partial index for <code>author_id</code> which covers only published articles, i.e., covers <code>WHERE published_at IS NOT NULL</code>!</p>

<p>Fortunately, this is supported by Rails (although writing a SQL command wouldn&rsquo;t be that difficult), we just need to use <code>where</code> option for that:</p>

<p><code>rb
add_column :articles, :author_id, where: "published_at IS NOT NULL"
</code></p>

<h3>Expression Indexes</h3>

<p>Imagine that you need to search users by their first name which comes from some input provided by a user. However, to avoid issues with figuring out whether the name provided by a user starts with a capital letter or not or how the names were stored in the database in the first place, you perform a query like this:</p>

<p><code>rb
User.where("lower(first_name) = ?", name.downcase)
</code></p>

<p>This query will obviously work, however, if you have a lot of users, a query plan will indicate that it is suboptimal and instead of seeing something like <code>Index Scan using index_users_on_first_name on users</code>, you will see <code>Seq Scan on users</code>.</p>

<p>There is no need to worry though. Postgres allows creating expression indexes where you can apply some functions, which in our case is <code>lower</code>. A proper index for this scenario would need to be created that way:</p>

<p><code>rb
add_index :users, "lower(first_name)", name: "index_users_on_lower_first_name"
</code></p>

<h3>Optimizing LIKE queries</h3>

<p>Optimizing queries with <code>LIKE</code> clause is simple; you just need to remember about two things:</p>

<ol>
<li>Forget about B-Tree Index for this case.</li>
<li>Take advantage of trigram matching provided by <a href="https://www.postgresql.org/docs/10/static/pgtrgm.html">pg_trgm</a> extension.</li>
</ol>


<p>To avoid sequential scans and utilize index that will drastically improve the performance of this kind of queries, enable the extension and create a GIN or GiST index:</p>

<p><code>
execute "CREATE EXTENSION pg_trgm;"
execute "CREATE INDEX CONCURRENTLY index_products_on_description_trigram ON clients USING gin(description gin_trgm_ops);"
</code></p>

<p>Thanks to this index, this is a query plan you might expect when filtering <code>Products</code> by <code>descriptions</code> containing some text, with wildcards on both the beginning and the end:</p>

<p>```
EXPLAIN for: SELECT &ldquo;products&rdquo;.* FROM &ldquo;products&rdquo; WHERE (products.description ILIKE &lsquo;%some text with wildacards%&rsquo;)</p>

<pre><code>                                            QUERY PLAN
</code></pre>

<hr />

<p> Bitmap Heap Scan on products
   Recheck Cond: (description ~~* &lsquo;%some text with wildacards%&rsquo;::text)
   &ndash;>  Bitmap Index Scan on index_products_on_description_trigram</p>

<pre><code>     Index Cond: (description ~~* '%some text with wildacards%'::text)
</code></pre>

<p>```</p>

<h3>Ordering</h3>

<p>B-tree indexes are sorted in an ascending order which we can use to our advantage to avoid performing sorting in memory. However, we also need to keep in mind the limitation of the data structure itself. A rule of thumb for efficient ordering would be: order by the same columns you perform filtering by. It is going to be the case by default when you don&rsquo;t explicitly add any <code>ORDER</code> clause since the indexes are ordered. But if it happens that you need to apply different ordering criteria, you can take advantage of <code>order</code> option and explicitly specify the order:</p>

<p><code>
add_index :products, :created_at, order: { created_at: :desc }
</code></p>

<h3>Adding indexes concurrently</h3>

<p>The way how the indexes are added doesn&rsquo;t impact the performance once they are created; however, it&rsquo;s good to keep in mind that just simple <code>CREATE INDEX</code> will block concurrent writes (inserts, updates, and deletes) until it&rsquo;s finished. It can lead to some issues, including deadlocks, especially when the index is getting created for a huge table under massive write operations.</p>

<p>To prevent such a problem, it&rsquo;s worth creating indexes <a href="https://www.postgresql.org/docs/10/static/sql-createindex.html#SQL-CREATEINDEX-CONCURRENTLY">concurrently</a> instead. You can do that in Rails using <code>algorithm: :concurrently</code> option and by making sure that the index creation will run outside of a transaction by calling <code>disable_ddl_transaction!</code>.</p>

<p>``` rb
class AddIndexToAsksActive &lt; ActiveRecord::Migration[5.0]
  disable_ddl_transaction!</p>

<p>  def change</p>

<pre><code>add_index :users, :active, algorithm: :concurrently
</code></pre>

<p>  end
end
```</p>

<p>There is one caveat here though. If you attempt to create a unique index concurrently, there is a possibility that something will go wrong, e.g., when a non-unique record is created during the index creation. Since the command is run outside the transaction, it won&rsquo;t be rolled back, and you will end up with an invalid index. Nevertheless, that is not a big problem &ndash; should it ever happen, just drop the invalid index and try creating it concurrently one more time.</p>

<h2>Wrapping up</h2>

<p>Optimizing <strong>PostgreSQL queries</strong> might not look a trivial task, but if you keep these rules in mind, you will have much easier time with your database, and you will enjoy fast queries for a long time.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Rails And Conditional Validations In Models]]></title>
    <link href="https://karolgalanciak.com/blog/2018/06/24/rails-and-conditional-validations-in-models/"/>
    <updated>2018-06-24T20:00:00+02:00</updated>
    <id>https://karolgalanciak.com/blog/2018/06/24/rails-and-conditional-validations-in-models</id>
    <content type="html"><![CDATA[<p>Adding consents for accepting Terms of Service/Privacy Policies must have been a top popular feature in the majority of the applications due to enforcement of <strong>GDPR</strong> in May ;). From the technical aspects that GDPR requires, there is a proof of consent for processing the personal information. In that case, you need to have some actual attributes in the database that would confirm the fact that some user has indeed accepted Terms of Service/Privacy Policy.</p>

<p>That makes a significant impact on how we approach this kind of features. However, in the past, such things were quite often not stored in a database at all &ndash; it just took some UI <strong>acceptance validation</strong> or maybe a <strong>validation of the virtual attribute</strong> on the backend to be on the safe side.</p>

<p>Let&rsquo;s focus on the latter case where we don&rsquo;t need to store anything in DB and see what the possible solutions to that problems are. As <strong>trivial</strong> as this problem initially sounds, it will get <strong>quite interesting</strong> ;).</p>

<!--more-->


<h2>Anatomy Of The Problem</h2>

<p>We want to make sure a user accepts Terms of Service during the signup process and to be sure that it is indeed validated, even if JavaScript validation fails in UI, we want to validate that fact on the backend.</p>

<h2>Solution 1 &ndash; Just add a virtual attribute to the model and validate it</h2>

<p>It is probably the most straightforward approach to that problem and most likely the least elegant. That&rsquo;s how we could implement it:</p>

<p>``` rb</p>

<h1>app/models/user.rb</h1>

<p>class User &lt; ApplicationRecord
  attr_accessor :terms_of_service_accepted</p>

<p>  validates :terms_of_service_accepted, acceptance: true
end
```</p>

<p>Well, it does work, no doubt about that. But currently, the validation will always be triggered, even during updates, which doesn&rsquo;t make much sense. We need to find a better solution.</p>

<h2>Solution 2 &ndash; Add a virtual attribute to the model and validate it only during the creation of a user</h2>

<p>A minor improvement over the previous version, we make sure that the validation is not triggered by updates, but only when creating a user:</p>

<p>``` rb</p>

<h1>app/models/user.rb</h1>

<p>class User &lt; ApplicationRecord
  attr_accessor :terms_of_service_accepted</p>

<p>  validates :terms_of_service_accepted, acceptance: true, on: :create
end
```</p>

<p>Even if it solves the actual problem, there is a big issue about that &ndash; the validation will always be triggered during a creation, even when creating users from factories! What other options do we have?</p>

<h2>Solution 3 &ndash; Add a virtual attribute to the model and validate it only for a specific context</h2>

<p>What is interesting in ActiveModel validations is that <code>on</code> option is not limited to <code>:create</code> or <code>:update</code> contexts &ndash; those are merely the ones that ActiveRecord sets by default depending on the persistence status of the model. We can provide a custom context for both <code>valid?</code> and <code>save</code> methods:</p>

<p><code>rb
user.valid?(:registration)
user.save(context: :registration)
</code></p>

<p>In that case, we could replace <code>:create</code> context with <code>:registration</code> context for the acceptance validation:</p>

<p>``` rb</p>

<h1>app/models/user.rb</h1>

<p>class User &lt; ApplicationRecord
  attr_accessor :terms_of_service_accepted</p>

<p>  validates :terms_of_service_accepted, acceptance: true, on: :registration
end
```</p>

<p>However, this is still not ideal &ndash; a global model which is used in multiple contexts has some logic that only applies to just one use case, and what is even worse, it&rsquo;s for an UI concern.</p>

<p>Let&rsquo;s try to find a solution that doesn&rsquo;t add any unnecessary mess to a model.</p>

<h2>Solution 4 &ndash; Use form object</h2>

<p>Using form object is probably the cleanest solution to our problem &ndash; we don&rsquo;t introduce any additional concerns to a model which should not be there, and we handle everything in a dedicated object. The are multiple ways how to implement a form object: we could create another ActiveModel model and take advantage of <a href="https://github.com/Azdaroth/active_model_attributes">ActiveModel Attributes</a> to make it smoother. We could use <a href="http://dry-rb.org/gems/dry-validation/basics/working-with-schemas/">dry-validation</a> gem for that. Or we could use my favorite tool for that purpose: <a href="https://github.com/trailblazer/reform">reform</a> gem from <a href="http://trailblazer.to">Trailblazer</a> stack.</p>

<p>Explaining the entire API of <code>reform</code> gem is way beyond the scope of this article, but the following implementation should be quite self-explanatory:</p>

<p>``` rb</p>

<h1>app/forms/user/registration_form.rb</h1>

<p>require &ldquo;reform/form/coercion&rdquo;</p>

<p>class User::RegistrationForm &lt; Reform::Form
  # other property declarations and validations</p>

<p>  property :terms_of_service_accepted, virtual: :true, type: Types::Form::Boolean</p>

<p>  validates :terms_of_service_accepted, acceptance: true
end
```</p>

<p>Besides handling other properties (most likely <code>email</code>, <code>password</code> and <code>password confirmation</code>), we are adding a virtual <code>terms_of_service_accepted</code> attribute with explicit type and adding acceptance validation using ActiveModel validator.</p>

<p>Even though using form objects is the cleanest approach, it requires some extra overhead, mostly with the setup, and sometimes it might be painful to add that setup, especially when extending third party&rsquo;s logic, e.g. <a href="https://github.com/scambra/devise_invitable">devise_invitable</a>. In such case, we would need some heavy customization which could potentially break when updating a gem and we would also need extra test coverage for the custom solution. It might still be worth introducing a form object, but it would be a good idea to consider other potential solutions. What option do we have left?</p>

<h2>Solution 5 &ndash; Extend user&rsquo;s instance with a custom logic</h2>

<p>Have you ever heard of DCI (Data Context Interaction) paradigm? If yes, you might have seen something like that:</p>

<p><code>rb
user = User.find(id)
user.extend(User::RegistrationContext)
</code></p>

<p>What this code does is adding extra functionality from <code>User::RegistrationContext</code> module to user&rsquo;s singleton class. Effectively, it means that we are not adding any additional logic to all User class instances, but only to that particular instance. Sounds like exactly what we need! That way, we can solve our problem achieving all the other goals as well &ndash; ease of extending the logic without too much overhead and without making a mess in the model.</p>

<p>Here is how our implementation of <code>User::RegistrationContext</code> context module could look like:</p>

<p>``` rb</p>

<h1>app/models/user/registration_context.rb</h1>

<p>module User::RegistrationContext
  def self.extended(model)</p>

<pre><code>class &lt;&lt; model
  validates :terms_of_service_accepted, acceptance: true
end
</code></pre>

<p>  end</p>

<p>  attr_accessor :terms_of_service_accepted
end
```</p>

<p>The interesting thing about this implementation is that there is some singletons' inception going on there &ndash; first, we are using <code>extend</code> itself on the model, and then, in <code>extended</code> module hook we are opening singleton class of the model and declaring validation there. However, this is necessary since <code>validates</code> method is not defined in the context of that module, and we need to do that in the context of the model.</p>

<p>Let&rsquo;s try our fancy solution in action:</p>

<p><code>rb
user = User.new
user.extend(User::RegistrationContext)
user.terms_of_service_accepted = "0"
user.valid?
=&gt; false
user.errors.messages[:terms_of_service_accepted]
=&gt; ["must be accepted"]
</code></p>

<p>Perfect, that&rsquo;s exactly what we needed!</p>

<h2>Wrapping Up</h2>

<p>There are multiple ways in Rails (or Ruby in general) to handle <strong>conditional validation</strong>, and thanks to the flexibility of the framework and the language, we can pick whatever seems best for our particular problem &ndash; from adding additional validations in a model with <strong>extra ActiveModel context</strong>, through using <strong>form objects</strong>, ending with arcane DCI-style object&rsquo;s extensions.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Case for before_validation callback: complex state normalization]]></title>
    <link href="https://karolgalanciak.com/blog/2018/05/27/the-case-for-before-validation-callback-complex-state-normalization/"/>
    <updated>2018-05-27T20:00:00+02:00</updated>
    <id>https://karolgalanciak.com/blog/2018/05/27/the-case-for-before-validation-callback-complex-state-normalization</id>
    <content type="html"><![CDATA[<p>Few months ago I wrote a <a href="https://karolgalanciak.com/blog/2017/10/29/the-case-against-exotic-usage-of-before-validate-callbacks/">blog post</a> about <strong>ActiveRecord</strong> <code>before_validation</code> callback and how it is used for  <strong>wrong reasons</strong> and concluded that in most cases this is not something we should be using routinely. However, I missed one <strong>appropriate use case</strong> for it which might be quite common in Rails apps, so this might be an excellent opportunity to get back to  <strong>before_validation callback</strong> and show its other side.</p>

<!--more-->


<h2>Anatomy Of The Problem</h2>

<p>Imagine that we have a <code>Payment</code> model where we need to store <code>amount</code> and <code>currency</code>. However, for statistical purposes, we also want to store normalized amount in USD currency with exchange rate applied at the time of payment&rsquo;s creation. As this is a significant part of our domain, we want to add validation for <code>amount_in_usd</code> attribute. Our Payment model looks like this at the moment:</p>

<p><code>rb
class Payment &lt; ApplicationRecord
  validates :amount, :currency, :amount_in_usd, presence :true
end
</code></p>

<p>The question is: where do we get <code>amount_in_usd</code> from and how can we assign it?</p>

<h2>The Solution</h2>

<p>One way of solving that problem would be a direct assignment when populating all the attributes. In that case, it would look a bit like this:</p>

<p><code>rb
Payment.new(currency: currency, amount: amount, amount_in_usd: CurrencyExchanger.exchange(amount, from: currency, to: "USD"))
</code></p>

<p>The problem with that solution is that this logic would need to be repeated in every place where payment gets initialized. We could implement a factory class that would be reused in all scenarios to keep it DRY, but that&rsquo;s some extra overhead that is not popular in a Rails world. Also, this sounds like a responsibility of the Payment model itself as it is about managing its internal state.</p>

<p>Here, we can&rsquo;t solve this by overriding writers as I suggested <a href="https://karolgalanciak.com/blog/2017/10/29/the-case-against-exotic-usage-of-before-validate-callbacks/">before</a> as <code>amount_in_usd</code> depends on two attributes: <code>currency</code> and <code>amount</code>, and we don&rsquo;t know in which sequence the attributes will be assigned.</p>

<p>And this is exactly the case where <code>before_validation</code> is useful: for complex state normalization where multiple attributes are involved. With that callback, a solution looks quite elegant and just simpler:</p>

<p>``` rb
class Payment &lt; ApplicationRecord
  validates :amount, :currency, :amount_in_usd, presence :true</p>

<p>  before_validation :assign_amount_in_usd</p>

<p>  private</p>

<p>  def assign_amount_in_usd</p>

<pre><code>if currency &amp;&amp; amount
  self.amount_in_usd = CurrencyExchanger.exchange(amount, from: currency, to: "USD")
end
</code></pre>

<p>  end
end
```</p>

<h2>Alternative Solution</h2>

<p>In the first paragraph, I mentioned that this solution could work especially well in Rails apps. What I meant by that is the fact that usually, the &ldquo;primitive&rdquo; attributes coming from HTTP params are mass-assigned to the model. Of course in Ruby, everything is an object, but to keep things simpler, let&rsquo;s treat numeric types and strings as primitives.</p>

<p>What would be a non-primitive value though? In our case, we have something that is widely used as a typical example of a value object: <strong>Money</strong> object that is composed of <code>amount</code> and <code>currency</code>.  If the attributes before the assignment were mapped to some more domain-oriented objects, we would have an even simpler solution for our problem:</p>

<p><code>rb
money = Money.new(amount, currency)
Payment.new(money: money)
</code></p>

<p>and the model would look like this:</p>

<p>``` rb
class Payment &lt; ApplicationRecord
  validates :amount, :currency, :amount_in_usd, presence :true</p>

<p>  def money=(money_object)</p>

<pre><code>self.amount = money_object.amount
self.currency = money_object.currency
self.amount_in_usd = CurrencyExchanger.exchange_money(money_object, to: "USD")
</code></pre>

<p>  end
end
```</p>

<p>It might look like extra overhead that is not necessary. However, value objects tend to simplify and DRY a lot of things in the code, so for more complex apps, using value objects will be worth that extra overhead.</p>

<h2>Wrapping Up</h2>

<p>There are some cases where <code>before_validation</code> callback might be useful. However, in more complex apps, using value object might be an alternative worth looking into.</p>
]]></content>
  </entry>
  
</feed>
