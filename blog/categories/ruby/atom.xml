<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Ruby | Karol Galanciak - Distributed Systems Architect and Ruby on Rails expert]]></title>
  <link href="https://karolgalanciak.com/blog/categories/ruby/atom.xml" rel="self"/>
  <link href="https://karolgalanciak.com/"/>
  <updated>2019-09-19T16:55:53+02:00</updated>
  <id>https://karolgalanciak.com/</id>
  <author>
    <name><![CDATA[Karol Galanciak]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Messages on Rails Part 3: RabbitMQ]]></title>
    <link href="https://karolgalanciak.com/blog/2019/06/23/messages-on-rails-part-3-rabbitmq/"/>
    <updated>2019-06-23T18:00:00+02:00</updated>
    <id>https://karolgalanciak.com/blog/2019/06/23/messages-on-rails-part-3-rabbitmq</id>
    <content type="html"><![CDATA[<p>In the <a href="https://karolgalanciak.com/blog/2019/02/24/messages-on-rails-part-1-introduction-to-kafka-and-rabbitmq/">first part</a> of this series, we were exploring some potential options for <strong>communication between services</strong> &ndash; what their advantages and disadvantages are, why <strong>HTTP API</strong> is not necessarily the best possible choice and suggesting that <strong>asynchronous messaging</strong> might be a better solution, using, e.g. <strong>RabbitMQ</strong> and <strong>Kafka</strong>. We&rsquo;ve already covered <strong>Kafka</strong> in the <a href="https://karolgalanciak.com/blog/2019/04/07/messages-on-rails-part-2-kafka/">part 2</a>, now it&rsquo;s the time for <strong>RabbitMQ</strong>.</p>

<!--more-->


<h2>What is RabbitMQ, and how does it work?</h2>

<p>RabbitMQ is a general purpose message broker supporting multiple protocols, yet, we are going to focus mostly on AMQP, which is the one that is the most typically used. It implements a smart broker/dumb consumer model. Unlike Kafka, it means that the broker is responsible for delivering messages to the right place. Also, the <code>queue</code> in Rabbit is what you would naturally consider being a queue &ndash; it&rsquo;s not database-like storage as Kafka topics are.</p>

<p>So what does publishing and consuming a message actually mean?</p>

<p>In the first step, a <code>producer</code> publishes a <code>message</code> &ndash; this can be a JSON message or whatever else that could be sent. This message is delivered to a <code>queue</code>, from which <code>consumers</code> can read it and process it. After the message is consumed, it&rsquo;s gone from the queue.</p>

<p>If this sounds to you like Redis and Sidekiq, you would be quite right. Actually, Redis could also be used as a message queue for communication between apps, although that might not be necessarily the best idea as being a message queue is not its primary purpose.</p>

<p>Why is RabbitMQ is so unique though, and what can it do that Redis or even Kafka can&rsquo;t do?</p>

<p>One of the key characteristics of RabbitMQ is that producers don&rsquo;t push messages directly to the queues. Instead, producers send messages to another layer called <code>exchanges</code>. This has quite profound consequences on the way RabbitMQ can be used and the features it offers:</p>

<ul>
<li>Producers don&rsquo;t need to be aware of all the queues where the message should be delivered, which means excellent decoupling between producers and consumers and ease of use</li>
<li>Without exchanges, it would not be possible to easily implement a publish-subscribe pattern where multiple consumers need to process a given message. Instead, you would have pretty much something like in Sidekiq &ndash; when the message is pushed to the queue, the workers compete for it, and it gets processed only once, there is no case of multiple workers handling the same message. However, competing consumers pattern is still possible in RabbitMQ (we will see it later but rather as something to avoid in that particular case).</li>
<li>Having extra layer in-between allows flexible routing and the implementation of some rules telling how and under what conditions consumers want to have messages delivered.</li>
</ul>


<p>In general, we could visualize the flow in RabbitMQ in the following way:</p>

<p class="img-center-wrapper">
  <img class="img-center" src="/images/messages_on_rails_part_3/rabbitmq_generalization.png" title="RabbitMQ" alt="RabbitMQ">
</p>




<p class="center">
  <a href="https://karolgalanciak.com/images/messages_on_rails_part_3/rabbitmq_generalization.png" target="_blank">See in better quality</a>
</p>


<h2>Publishing/Receiving messages and exchanges</h2>

<p>Ability to implement a publish-subscribe pattern is a natural consequence of having exchanges. As we could see on the graph above, the producer is not aware of all of the queues to which the messages will be delivered. It might be even the case that there are no queues at all that would be interested in any message!</p>

<p>For the queues to receive messages, we need to create a relationship between that queue and the exchange. In RabbitMQ it&rsquo;s called a <code>binding</code>.</p>

<p>So far, we know that the publisher sends a message to the exchange and exchange delivers the messages to queues that are bound to it. But what are the rules here that would determine to which queues exactly the message should be delivered to (besides the fact that the queues must have established a binding with the exchange)?</p>

<p>To achieve that, we have four types of exchange types that have a bit different behavior:</p>

<ol>
<li><code>fanout</code> exchange &ndash; it just delivers a message to all the queues that are bound to it.</li>
<li><code>direct</code> &ndash; a given queue might be only interested in some particular type of messages that could be filtered out by some criteria. E.g., you might be publishing logs with different levels, just like in <code>Rails.logger</code> where you have <code>debug</code>, <code>info</code>, <code>fatal</code> etc. severity levels. In that case, there might be a queue that wants to receive only <code>fatal</code> logs, and another one might want to receive just <code>debug</code> logs. Instead of letting the consumer do the filtering itself, we can let RabbitMQ do it by specifying a <code>routing key</code> in the message. We could publish logs with routing keys such as <code>log.debug</code> and <code>log.fatal</code> and let the queues bind accordingly using that routing key as a <code>binding key</code>. That&rsquo;s exactly what a <code>direct</code> exchange is about. However, what if we want to have a third queue that would be interested in all types of logs? In that case, we would need to use another kind of exchange:</li>
<li><code>topic</code> &ndash; it&rsquo;s the exchange that allows receiving messages based on the patterns. In case of the example of logs, the queue that would be interested in all kinds of logs could create a binding with <code>log.*</code> routing key. And what if it were interested in receiving all messages (i.e., work as a <code>fanout</code> exchange, even though we are using <code>topic</code> exchange here)? RabbitMQ has us covered, we could use a special routing key: <code>#</code> and that would be enough. Also, the <code>topic</code> exchange can also act as a <code>direct</code> one &ndash; there is nothing preventing a queue from binding with a full routing key name, so using more &ldquo;advanced&rdquo; exchange type is only adding more options and flexibility, it doesn&rsquo;t take away the features of &ldquo;simpler&rdquo; ones.</li>
<li><code>headers</code> exchange type &ndash; arguably a less common one, it&rsquo;s quite similar to <code>topic</code> exchange, although the filtering is performed based on the values of the headers instead of routing keys.</li>
</ol>


<h2>Using RabbitMQ in Ruby/Rails apps</h2>

<h3>RPC with RabbitMQ</h3>

<p>There is one somewhat unexpected feature of RabbitMQ (at least if you focus on async messaging) &ndash; Remote Procedure Call (RPC). That way you can implement synchronous request-reply flow, just like with HTTP APIs. So if it&rsquo;s just like good ol' HTTP API, why would you even consider RPCs via RabbitMQ?</p>

<p>Sometimes you might not need REST API as you might be interested only in executing a specific set of commands/procedures in another service that don&rsquo;t easily map to resources that you would expose in REST API. It is especially true if you develop a private API which is not going to be exposed to Third Party Users, so there is little concern for &ldquo;externalization&rdquo;.</p>

<p>Let&rsquo;s check a quick example to see it in action. To make RPC with RabbitMQ what we need to do is to publish a message and specify a reply-to (callback) queue to which the server will reply.</p>

<p>Let&rsquo;s implement a classic: a Fibonacci sequence server to which the clients will be sending the numbers and expecting the result in the response. One of the super simple ways to implement RPCs in Ruby is to do it via <a href="https://github.com/peejaybee/bunny_burrow">bunny_borrow gem</a>. To see it almost right away in action, you could add it to a Gemfile of any Rails application, open two Rails console processes, execute this code in the first one:</p>

<p>``` rb
class Fibonacci
  def self.calculate(number, accum = {})</p>

<pre><code>return number if number == 0 || number == 1
accum[number] ||= calculate(number - 1, accum) + calculate(number-2, accum)
</code></pre>

<p>  end
end</p>

<p>rpc_server = BunnyBurrow::Server.new do |server|
  server.rabbitmq_url = &ldquo;amqp://guest:guest@localhost:5672&rdquo;
  server.rabbitmq_exchange = &ldquo;fibonacci_sequence&rdquo;
  server.logger = Logger.new(STDOUT)
end</p>

<p>rpc_server.subscribe(&ldquo;fibonacci.route&rdquo;) do |payload|
  BunnyBurrow::Server.create_response.tap do |response|</p>

<pre><code>response[:result] = Fibonacci.calculate(JSON.parse(payload).fetch("number"))
</code></pre>

<p>  end
end</p>

<p>rpc_server.wait
```</p>

<p>and this in the second one:</p>

<p>``` rb
rpc_client = BunnyBurrow::Client.new do |client|
  client.rabbitmq_url =  &ldquo;amqp://guest:guest@localhost:5672&rdquo;
  client.rabbitmq_exchange = &ldquo;fibonacci_sequence&rdquo;
  client.logger = Logger.new(STDOUT)
end</p>

<p>payload = { &ldquo;number&rdquo; => 10 }</p>

<p>response = rpc_client.publish(payload, &ldquo;fibonacci.route&rdquo;)
puts JSON.parse(response)[&ldquo;result&rdquo;]
```</p>

<p>Just don&rsquo;t forget to install RabbitMQ before ;).</p>

<p>And that&rsquo;s it! You will see the result of <code>55</code> getting printed. RPC with <code>bunny_borrow</code> merely requires binding to the right exchange, agreeing on the routing key we are going to use and publishing a message!</p>

<h3>Background jobs with Sneakers &ndash; Redis/Sidekiq replacement</h3>

<p>As already mentioned before, RabbitMQ can be a bit like Redis for background processing purposes. And it turns out that we even have a reliable replacement for Sidekiq that is backed by RabbitMQ! We can easily implement workers with <a href="https://github.com/jondot/sneakers">sneakers gem</a>, especially if you use ActiveJob as we have already adapter for Sneakers! But even if you use Sneakers standalone, you shouldn&rsquo;t expect too many surprises there. It looks like a regular worker class. The only difference is that it requires manual acknowledgment of having the message processed:</p>

<p>``` rb
class MySneakersWorker
  include Sneakers::Worker
  from_queue :queue_name</p>

<p>  def work(payload)</p>

<pre><code>Rails.logger.info "From Sneakers: #{payload}"

ack!
</code></pre>

<p>  end
end
```</p>

<p>If you are curious why you might want to prefer Sneaker over Sidekiq, I would recommend reading the <a href="https://github.com/jondot/sneakers/wiki/Why-i-built-it">docs</a>.</p>

<h3>Event-Driven Applications with Hutch</h3>

<p>The case where RabbitMQ really will shine in our Rails applications is for event-driven architecture or in general, publish-subscribe use cases. It happens quite often in communication between microservices that we don&rsquo;t necessarily need synchronous calls between two or more apps and making it async will improve the efficiency, scalability and also will allow making the apps more independent, coherent and less coupled to each other. For example, if one application is down, we don&rsquo;t need to worry about this as our message will be processed eventually when the app is back. For synchronous flow, that would be very different as we would need to implement error handling and simply be prepared for that kind of scenario. Also, quite often, there is a clear distinction between primary logic behind some use cases and secondary logic.</p>

<p>Consider the following scenario: you have a checkout page in your e-commerce system where a user can enter credit card data and pay for the order. Are all the following &ldquo;features&rdquo; equally important?</p>

<ul>
<li>executing the charge and confirming the order</li>
<li>sending an email notification to the buyer</li>
<li>sending WhatsApp message someone who will be responsible for delivering this order</li>
<li>updating some internal stats about the orders</li>
</ul>


<p>The first one is clearly the critical part of the logic, and the rest looks like some extra aspects that happen after the order is paid. Should they be called from the same place? And how does it go with the Single Responsibility Principle?</p>

<p>Let&rsquo;s visualize this scenario with the following pseudocode:</p>

<p>``` rb
class Order::ConfirmByPayment
  def call(order, payment)</p>

<pre><code>order.confirm!(payment)
send_notification_to_the buyer(order)
send_whatsapp_message_for_delivery(order)
update_stats_for_orders(order)
update_stats_for_payment(payment)
</code></pre>

<p>  end</p>

<p>  # private methods go here
end
```</p>

<p>Certainly, this class looks too busy as it is now. And what if we introduced some extra notifications or other aspects? Would we add just another private method? It doesn&rsquo;t sound exactly like something compliant with SRP.</p>

<p>So what options do we have? One way to deal with it is to go with publish-subscribe pattern &ndash; publish some domain event and implement subscribers that would react to this event. It is doable in Rails apps without too much hassle with <a href="https://github.com/krisleech/wisper">wisper</a> gem. Explaining how to use <code>wisper</code> is outside the scope of this article, but to cut a long story short, that&rsquo;s what this service would look like after a rewrite to event-driven architecture:</p>

<p>``` rb
class Order::ConfirmByPayment
  include Wisper::Publisher</p>

<p>  def call(order, payment)</p>

<pre><code>order.confirm!(payment)
broadcast(:order_confirmed_by_payment, order.as_json, payment.as_json)
</code></pre>

<p>  end
  # private methods go here
end
```</p>

<p>Each of the notification could be extracted to a separate class that is subscribed to <code>order_confirmed_by_payment</code> event. And that way, we would get clean and decoupled classes where the logic is easy to extend.</p>

<p>What does it have to do with RabbitMQ though?</p>

<p>In the code we&rsquo;ve just gone through, everything is implemented within the same application. However, this is not really necessary. Sending notifications can be async, and it&rsquo;s not needed to be done from the same application, we could have a separate service for stats, for sending email notifications, for sending WhatsApp messages etc. And for that, we need some simple way how to inform all the applications that they should do something. And that&rsquo;s where we will need RabbitMQ and one of the Ruby &ldquo;frameworks&rdquo; that is built for this kind of use cases.</p>

<h4>Building Example Producer and Consumer With RabbitMQ and Hutch</h4>

<p>We are going to use <a href="https://github.com/gocardless/hutch">hutch</a> gem to publish event <code>order.confirmed_by_payment</code> event so that some consumers (either within the same app or from apps, it doesn&rsquo;t make much difference) can do something upon it.</p>

<p>Hutch itself is quite opinionated, so there is not much left for us to figure out, yet, it&rsquo;s good to keep in mind that it uses <code>topic</code> exchanges.</p>

<p>Let&rsquo;s get back to our <code>Order::ConfirmByPayment</code> and publish a message with Hutch from there:</p>

<p>``` rb
class Order::ConfirmByPayment
  def call(order, payment)</p>

<pre><code>order.confirm!(payment)

Hutch.connect
Hutch::Config.set(:force_publisher_confirms, true)

Hutch.publish("order.confirmed_by_payment", payment: payment.as_json, order: order.as_json)
</code></pre>

<p>  end
end
```</p>

<p>The logic is quite simple: first, we need to connect Hutch. That&rsquo;s probably not the place where it should be executed, some initializer might be a better idea, but it&rsquo;s in this class to keep the example simpler. It&rsquo;s the same thing with <code>force_publisher_confirms</code> option (that should be moved to some initializer as well) which we use here for extra durability and better guarantees as we require the publisher to wait for the confirmation that the message was published. And then, we publish our message with <code>payment</code> and <code>order</code> keys and their serialized projections as values under <code>order.confirmed_by_payment</code> routing key.</p>

<p>To consume the messages that are published by this class, we need some consumer classes:</p>

<p>``` rb
class Order::ConfirmedByPaymentStatsUpdaterConsumer
  include Hutch::Consumer
  consume &ldquo;order.confirmed_by_payment&rdquo;</p>

<p>  def process(message)</p>

<pre><code>do_something_with_payment_and_order(message.body[:payment], message.body[:order])
</code></pre>

<p>  end
end
```</p>

<p>``` rb
class Order::ConfirmedByPaymentWhatsAppNotifierConsumer
  include Hutch::Consumer
  consume &ldquo;order.confirmed_by_payment&rdquo;</p>

<p>  def process(message)</p>

<pre><code>do_something_with_payment(message.body[:payment])
</code></pre>

<p>  end
end
```</p>

<p>``` rb
class Order::ConfirmedByPaymentEmailNotifierConsumer
  include Hutch::Consumer
  consume &ldquo;order.confirmed_by_payment&rdquo;</p>

<p>  def process(message)</p>

<pre><code>do_something_with_order(message.body[:order])
</code></pre>

<p>  end
end
```</p>

<p>And that&rsquo;s it! What we need to do is to define consumer classes that include <code>Hutch::Consumer</code> module, define the routing key under which we are going to consume messages and implement <code>process</code> method that will deal with the <code>message</code> that we get and do something with its <code>body</code> that contains what we published via publisher.</p>

<p>Optionally, we may want to specify <code>queue_name</code> explicitly, which is by default generated based on the class name. It will work without an explicit declaration for the majority of the cases, but what is going to happen if we have a consumer with the same class name in more than one application? Instead of having a publish-subscribe pattern, we are going to end up with competing consumers as having the same queue names is the way to do in RabbitMQ! In that case, it&rsquo;s worth prefixing queue name with the app&rsquo;s name or something else that would make it unique in your ecosystem:</p>

<p>``` rb
class Order::ConfirmedByPaymentWhatsAppNotifierConsumer
  include Hutch::Consumer
  consume &ldquo;order.confirmed_by_payment&rdquo;
  queue_name &ldquo;notifs_app_order_confirmed_by_payments_whats_app_notifier&rdquo;</p>

<p>  def process(message)</p>

<pre><code>do_something_with_payment(message.body[:payment])
</code></pre>

<p>  end
end
```</p>

<p>The last thing to do is to start the worker process that will consume the messages:</p>

<p><code>
bundle exec hutch
</code></p>

<p>You should see an output that looks like this:</p>

<p><code>
2019-06-15T19:27:52Z 84284 INFO -- hutch booted with pid 84284
2019-06-15T19:27:52Z 84284 INFO -- found rails project (.), booting app in development environment
2019-06-15T19:27:57Z 84284 INFO -- connecting to rabbitmq (amqp://guest@127.0.0.1:5672/)
2019-06-15T19:27:57Z 84284 INFO -- connected to RabbitMQ at 127.0.0.1 as guest
2019-06-15T19:27:57Z 84284 INFO -- opening rabbitmq channel with pool size 1, abort on exception false
2019-06-15T19:27:57Z 84284 INFO -- using topic exchange 'hutch'
2019-06-15T19:27:57Z 84284 INFO -- HTTP API use is enabled
2019-06-15T19:27:57Z 84284 INFO -- connecting to rabbitmq HTTP API (http://guest@127.0.0.1:15672/)
2019-06-15T19:27:57Z 84284 INFO -- tracing is disabled
2019-06-15T19:27:57Z 84284 INFO -- setting up queues
</code></p>

<p>And you can enjoy using RabbitMQ in your apps and having the logic nicely decoupled with a little cost ;)</p>

<h2>Wrapping Up</h2>

<p><strong>RabbitMQ</strong> is an excellent choice if you need <strong>multiple applications</strong> to talk to each other via events instead of HTTP requests/responses and should you ever need <strong>synchronous</strong> calls, you could take advantage of its <strong>RPC</strong> capabilities. It&rsquo;s a mature <strong>message broker</strong> with well-established Ruby gems that will make it simple to introduce it in your Rails apps.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Messages on Rails Part 2: Kafka]]></title>
    <link href="https://karolgalanciak.com/blog/2019/04/07/messages-on-rails-part-2-kafka/"/>
    <updated>2019-04-07T20:00:00+02:00</updated>
    <id>https://karolgalanciak.com/blog/2019/04/07/messages-on-rails-part-2-kafka</id>
    <content type="html"><![CDATA[<p>In the <a href="https://karolgalanciak.com/blog/2019/02/24/messages-on-rails-part-1-introduction-to-kafka-and-rabbitmq/">first part</a> of this series, we were exploring some potential options for <strong>communication between services</strong> &ndash; what their advantages and disadvantages are, why <strong>HTTP API</strong> is not necessarily the best possible choice and suggesting that <strong>asynchronous messaging</strong> might be a better solution, using, e.g. <strong>RabbitMQ</strong> and <strong>Kafka</strong>. Let&rsquo;s focus this time entirely on the latter.</p>

<!--more-->


<h2>What Is Kafka?</h2>

<p>Kafka is a distributed streaming platform which allows the implementation of a publish-subscribe model between producers and consumers. However, what is unique about Kafka, is the fact that it&rsquo;s somewhat closer to a storage system than a message queue. Unlike in a typical message queue, the messages are not removed once they are consumed, they are still kept there. But what exactly is that &ldquo;there&rdquo;? The events are stored in topics &ndash; the append-only logs in which messages are identified by &ldquo;offsets&rdquo;, which you could compare to array indexes. Topics can be split for parallelization by a specified key forming a partition.</p>

<p><img src="/images/messages_on_rails_part_2/kafka_topics.png" title="&lsquo;Kafka Topics&rsquo; &lsquo;Kafka Topics&rsquo;" ></p>

<p class="center">
  <a href="https://karolgalanciak.com/images/messages_on_rails_part_2/kafka_topics.png" target="_blank">See in better quality</a>
</p>


<p>What is more, a broker is not responsible for delivering the messages to the consumers &ndash; it is a responsibility of the consumer to make sure they consume the messages and keep track of the offsets so that they can read messages one by one.</p>

<p>There are notable side-effects of that design which gives a lot of options when it comes to taking advantage of Kafka. Since we deal with append-only logs, all the messages under a given partition in the topic are ordered. Also, as the messages are persistent and keeping track of the position in the log is on consumers, the events can be replayed by any consumer, including the new ones that have been just introduced to the ecosystem. When using a traditional message queue, there will be no way to access past messages when a new consumer joins, and you would need to do things like resending events manually or whatever else would make the most sense in that scenario, but with Kafka, this is something totally normal.</p>

<p>Since the messages are persisted, storage can become an issue on its own, What if our producers publish a massive number of events and we are not interested in keeping all of them, or we want to limit somehow the number of events that will be consumed by future consumers without actually losing anything important?</p>

<p>The first use case is quite simple &ndash; we may configure retention policy based on time (and, e.g., delete everything that is older than 30 days) or based on size (keep 1 TB of the logs). However, that might mean losing potentially valuable data.</p>

<p>That issue can be addressed differently, using log compaction. Imagine that you are publishing the projections of the models after each change to Kafka topics. If you are not interested in how exactly you ended up with having records in a particular state but only their latest state, there is clearly no need to store anything that happened before the last update. In that case, you can decide to compact logs by a given key achieving precisely that result. Moreover, you can also get rid of the message under a given key altogether &ndash; just send a message with the null payload for a specific key, and that&rsquo;s it! No issues with storage and no need to consume messages in the future to figure out in the end that some record was deleted and a lot of processing went for nothing.</p>

<h2>Modelling Kafka Topics</h2>

<p>It seems like modeling topics shouldn&rsquo;t be rocket science as we have limited options &ndash; we have topics where we can publish messages, and those topics can be parallelized by partitioning. However, this issue might be quite complex.</p>

<p>There are a couple of things that we need to take into consideration when designing topics:</p>

<ul>
<li>Messages are ordered within the same topic/partition</li>
<li>The more partitions we have, the higher the throughput we can achieve</li>
<li>The number of partitions per node has its limits which are determined by the number of file descriptors the operating system can handle since each partition is represented as a separate file. This value is configurable, yet, it is essential to keep that limit in mind and that you cannot increase it indefinitely, so having millions of partitions on a single node won&rsquo;t work</li>
<li>Also, more partitions increase latency</li>
<li>The number of topics is limited in the same way as the number of partitions. In that sense, 10 000 topics with a single partition will not be that different from 1 topic with 10 000 partitions</li>
</ul>


<p>A first idea that you might have in mind when designing topics is to have a topic per model, or generally speaking, a topic for events coming from the same model. That way, you might go with <code>orders</code> topic where you would publish events like <code>order_created</code>, <code>order_completed</code>, <code>order_canceled</code> etc.</p>

<p>There are several problems with that approach though. What if you have an event-driven system, where a consumer immediately acts on the data it receives? Let&rsquo;s say that you have invoicing microservice that sends invoices to customers upon <code>order_completed</code> events. Obviously, besides the data from orders, it requires the data from customers. What if a customer changes the shipping addresses just before the completion of the order (resulting in publishing <code>customer_shipping_address_updated</code> event), yet, the <code>order_completed</code> event was processed before since the messages are published to different topics and they get processed independently? It might be the case that the address on the invoice will be incorrect! You might protect yourself against this kind of issues by assuming eventual consistency and, e.g., not acting on <code>order_completed</code> immediately but waiting 5 minutes instead, but this is not a solution that will work in all the cases and sometimes the cost of having issues like that might be too big, and you just can&rsquo;t afford this.</p>

<p>In such case, if ordering matters, it is a better idea to publish events coming from different models to the same topic (and to the same partition!). In general, we can say that things that are needed together (order with their items and customer data) belong to the same topic/partition.</p>

<p>What if different consumers have different needs as far as ordering goes and what things should go together? It is a more tricky scenario, and there is no one-size-fits-all solution, you would need to make some tradeoffs, either on a producer or consumer(s) side.</p>

<p>Let&rsquo;s get back to our example and imagine that we have another service that is for some reason interested only in customers' data. Such an application would clearly be interested in <code>customer_shipping_address_updated</code> events, but they don&rsquo;t care at all about anything related to orders. That scenario is pretty simple as the solution might be just ignoring the events (i.e., doing nothing upon them) that the service is not interested in. However, what if that would mean ignoring 90% of the events from the topic? You may choose to publish the same kind of events to different topics, so that they are easier to consume by different consumers, so the extra overhead would be on the producer&rsquo;s side. There is a possibility of different scenario: another service has even more strict requirements about the ordering of more kinds of events. The solution would be no different though &ndash; either we could publish events to another topic, or we could use the same one and let the other consumers ignore even more events.</p>

<p>There is one more possible scenario: what if <code>order_completed</code> and <code>customer_shipping_address_updated</code> are a result of the same request/transaction? In such case, we could have a single event, e.g., <code>order_completed_with_customer_address_change</code> that would have a more complex payload and later, that event could be decomposed to smaller ones like <code>order_completed</code> and <code>customer_shipping_address_changed</code> and published to separate topics using a stream processor. Maybe this is a bit contrived scenario, yet, it&rsquo;s worth keeping in mind that decomposition of events may also be a viable option, which is a much better choice than publishing split events and trying to reconstruct the original one.</p>

<h2>Stream Processing With KSQL/Apache Spark/Apache Storm/Apache Samza/Apache Flink, oh my!</h2>

<p>Even if you are not deep into data science, you&rsquo;ve probably heard about Hadoop, which is arguably a king of batch processing in a big data world. However, that is not necessarily the most optimal solution if you need an almost real-time data processing which ideally requires working with streams (or at least with micro-batches). And this is exactly what we have with Kafka!  There is a wide range of tools that can work with Kafka easily that could be used for ETL (Extract, Transform, Load), Data Enrichment, Machine Learning, Interactive Analysis or even as an extra component of Event Sourcing (e.g., building projections of the models from all the streams of events). Depending on the use case and requirements (e.g. very low latency, ease of use), you could go with <a href="https://kafka.apache.org/documentation/streams/">Kafka Streams</a>, <a href="https://www.confluent.io/product/ksql/">KSQL</a>, <a href="https://spark.apache.org">Apache Spark</a>, <a href="http://storm.apache.org">Apache Storm</a>, <a href="http://samza.apache.org">Apache Samza</a>, <a href="https://flink.apache.org">Apache Flink</a> or yet another solution. The list is quite long, and each of these tools deserves a separate article which is out of the scope of this one. Yet, it&rsquo;s good to be aware of how introducing Kafka opens the way to more possibilities that are data-oriented.</p>

<h2>Building Example Producer and Consumer Apps With Karafka</h2>

<p>Now, time for some practice! Let&rsquo;s build an example producer/consumer app that produces and consumes some event. To keep the example simple, let&rsquo;s just build a single application as it will be no different than having a producer and consumer as separation applications.</p>

<p>We are going to use <a href="https://github.com/karafka/karafka">karafka</a> gem for that, which is a higher level framework that structures the way of producing and consuming events and allows to focus on the business logic instead of figuring out how to integrate Kafka with the rest of the application.</p>

<h3>Getting Started</h3>

<p>Let&rsquo;s generate a new application:</p>

<p><code>
rails new karafka_example
</code></p>

<p>And add required gems to Gemile:</p>

<p><code>rb
gem "waterdrop", git: "https://github.com/karafka/waterdrop.git", branch: "master"
gem "karafka", git: "https://github.com/karafka/karafka.git", branch: "master"
</code></p>

<p>Besides &ldquo;karafka&rdquo; gem, we also have &ldquo;waterdrop&rdquo; gem which is a dependency required for publishing events.</p>

<p>Let&rsquo;s run a generator for all the things we need from &ldquo;karafka&rdquo;:</p>

<p><code>
bundle exec karafka install
</code></p>

<p>That command should generate a couple of interesting files: <code>karafka.rb</code> in a root directory, <code>app/consumers</code> with <code>application_consumer.rb</code> file and <code>app/</code>responders directory with <code>application_responder.rb</code> file.</p>

<p>The first file is pretty much an initializer for Karafka application which is separated from Rails config and allows you to configure Karafka application and to draw routes, which is similar in terms of API as Rails application routes, but here, it&rsquo;s for topics and consumers.</p>

<p><code>ApplicationConsumer</code> class is what you would expect that class to be &ndash; a base class for consumers. The interface is quite straight-forward for basic use cases: we define <code>consume</code> instance method inside which we have available <code>params</code> or <code>params_batch</code> (if we enabled batch fetching for that topic) that give access to event&rsquo;s payload. There are also some callbacks available for consumers which you can check in the <a href="https://github.com/karafka/karafka/wiki/Consumer-callbacks">docs</a>.</p>

<p><code>ApplicationResponder</code> is a base class for producers. Why &ldquo;responder&rdquo; then and not a producer? Mostly due to the fact that they are quite similar to responders that you might have used in controllers as far as the intention goes. Usually, inside a responder, you declare a topic that you want to have registered so that you can send and consume events from it. Next, you define <code>respond</code> instance method which takes a single argument which is the event payload. From that method, you call <code>respond_with</code> method where you can provide topic name, a payload of the event and optionally, a partition key.</p>

<h3>Initializer</h3>

<p>Here&rsquo;s an initializer generated by <code>karafka</code> generator with some extra tweaks:</p>

<p>``` rb karafka.rb</p>

<h1>frozen_string_literal: true</h1>

<p>ENV[&lsquo;RAILS_ENV&rsquo;] ||= &lsquo;development&rsquo;
ENV[&lsquo;KARAFKA_ENV&rsquo;] = ENV[&lsquo;RAILS_ENV&rsquo;]
require ::File.expand_path(&lsquo;../config/environment&rsquo;, <strong>FILE</strong>)
Rails.application.eager_load!</p>

<h1>This lines will make Karafka print to stdout like puma or unicorn</h1>

<p>if Rails.env.development?
  Rails.logger.extend(</p>

<pre><code>ActiveSupport::Logger.broadcast(
  ActiveSupport::Logger.new($stdout)
)
</code></pre>

<p>  )
end</p>

<p>class KarafkaApp &lt; Karafka::App
  setup do |config|</p>

<pre><code>config.kafka.seed_brokers = %w[kafka://127.0.0.1:9092]
config.client_id = "karafka_example"
config.backend = :inline
config.batch_fetching = false
# Uncomment this for Rails app integration
# config.logger = Rails.logger
</code></pre>

<p>  end</p>

<p>  # Comment out this part if you are not using instrumentation and/or you are not
  # interested in logging events for certain environments. Since instrumentation
  # notifications add extra boilerplate if you want to achieve max performance,
  # listen to only what you really need for a given environment.
  Karafka.monitor.subscribe(Karafka::Instrumentation::StdoutListener)
  Karafka.monitor.subscribe(Karafka::Instrumentation::ProctitleListener)</p>

<p>  consumer_groups.draw do</p>

<pre><code>consumer_group :example do
  batch_fetching false

  topic :users do
    consumer UsersConsumer
  end
end
</code></pre>

<p>  end
end</p>

<p>Karafka.monitor.subscribe(&lsquo;app.initialized&rsquo;) do
  # Put here all the things you want to do after the Karafka framework
  # initialization
end</p>

<p>KarafkaApp.boot!
```
The most interesting parts of that file are configuration and drawing the routes, so let&rsquo;s focus on them now.</p>

<p>In the configuration block, we need to specify URI for Kafka, which is going to be kafka://127.0.0.1:9092 by default (assuming that you have Kafka installed in the first place). We also need <code>client_id</code> to identify the app in Kafka and provide the proper namespace. Another option is specifying <code>backend</code> which we have set to be <code>inline</code>. It means that the processing of the events will happen in the karafka&rsquo;s workers, which might not be an optimal solution, especially for heavy processing. Alternatively, we can use Sidekiq thanks to <a href="https://github.com/karafka/sidekiq-backend">karafka sidekiq-backend gem</a>. Keep in mind though that with Sidekiq backend the the messages might no longer be processed in the order they arrived! For the sake of simplicity and the demonstration purposes, <code>batch_processing</code> is set to false. In such case, instead of dealing with a batch of messages (that would be available under <code>params_batch</code> method), we are going to have just <code>params</code> representing a single message.</p>

<p>In the next section, we are configuring the equivalent of routes for Kafka. We are registering <code>example</code> consumer group under which we specify that we want <code>UsersConsumer</code> class to be the consumer for <code>users</code> topic.</p>

<p>Consumers group is a concept that allows having competing consumers pattern for messages, which is useful if you don&rsquo;t want to have a publish-subscribe pattern with all consumers receiving the same message but only a single consumer processing the event.</p>

<p>There are a couple of more configuration options for topics that you can check in the <a href="https://github.com/karafka/karafka/wiki/Routing#topic-level-options">docs</a>.</p>

<h3>Building a producer</h3>

<p>Let&rsquo;s build the simplest possible producer that will merely take the payload and publish it to <code>users</code> topic:</p>

<p>``` rb app/responders/users_responder.rb
class UsersResponder &lt; ApplicationResponder
  topic :users</p>

<p>  def respond(event_payload)</p>

<pre><code>respond_to :users, event_payload
</code></pre>

<p>  end
end
```</p>

<h3>Building a consumer</h3>

<p>And now, we can build a simple producer that will do nothing more than logging the params. As mentioned before, we have <code>params</code> method accessible on the instance level (or <code>params_batch</code> if batch fetching is enabled):</p>

<p>``` rb app/consumers/users_consumers.rb
class UsersConsumer &lt; ApplicationConsumer
  def consume</p>

<pre><code>Karafka.logger.info "New [User] event: #{params}"
</code></pre>

<p>  end
end
```</p>

<h3>Seeing things in action</h3>

<p>Let&rsquo;s start the server process:</p>

<p><code>text
bundle exec karafka server
</code></p>

<p>If everything goes fine, you should see an output like this:
<code>text
I, [2019-04-07T10:30:41.517728 #83821]  INFO -- : Initializing Karafka server 83821
Karafka framework version: 1.3.0
Application client id: karafka_example
Backend: inline
Batch fetching: false
Batch consuming: false
Boot file: /path/to/app/karafka.rb
Environment: development
Kafka seed brokers: ["kafka://127.0.0.1:9092"]
I, [2019-04-07T10:30:41.602016 #83821]  INFO -- : Running Karafka server 83821
I, [2019-04-07T10:30:41.602870 #83821]  INFO -- : New topics added to target list: users
I, [2019-04-07T10:30:41.602948 #83821]  INFO -- : Fetching cluster metadata from kafka://127.0.0.1:9092
I, [2019-04-07T10:30:41.798673 #83821]  INFO -- : Discovered cluster metadata; nodes: your-machine-name.local:9092 (node_id=0)
I, [2019-04-07T10:30:41.798965 #83821]  INFO -- : [[karafka_example_example] {}:] Will fetch at most 1048576 bytes at a time per partition from users
I, [2019-04-07T10:30:41.799144 #83821]  INFO -- : [[karafka_example_example] {}:] Fetching cluster metadata from kafka://127.0.0.1:9092
I, [2019-04-07T10:30:41.799005 #83821]  INFO -- : [[karafka_example_example] {}:] Joining group `karafka_example_example`
I, [2019-04-07T10:30:41.801184 #83821]  INFO -- : [[karafka_example_example] {}:] Discovered cluster metadata; nodes: your-machine-name.local:9092 (node_id=0)
I, [2019-04-07T10:30:41.801287 #83821]  INFO -- : [[karafka_example_example] {}:] There are no partitions to fetch from, sleeping for 1s
I, [2019-04-07T10:30:42.047094 #83821]  INFO -- : [[karafka_example_example] {}:] Joined group `karafka_example_example` with member id `karafka_example-612b6612-26f1-41fe-95cd-49a52a6275c7`
I, [2019-04-07T10:30:42.047186 #83821]  INFO -- : [[karafka_example_example] {}:] Chosen as leader of group `karafka_example_example`
I, [2019-04-07T10:30:42.803209 #83821]  INFO -- : [[karafka_example_example] {}:] There are no partitions to fetch from, sleeping for 1s
I, [2019-04-07T10:30:43.050550 #83821]  INFO -- : [[karafka_example_example] {}:] Fetching cluster metadata from kafka://127.0.0.1:9092
I, [2019-04-07T10:30:43.052574 #83821]  INFO -- : [[karafka_example_example] {}:] Discovered cluster metadata; nodes: your-machine-name.local:9092 (node_id=0)
I, [2019-04-07T10:30:43.106051 #83821]  INFO -- : [[karafka_example_example] {}:] Partitions assigned for `users`: 0
I, [2019-04-07T10:30:43.805927 #83821]  INFO -- : [[karafka_example_example] {users: 0}:] Seeking users/0 to offset 0
I, [2019-04-07T10:30:43.806048 #83821]  INFO -- : [[karafka_example_example] {users: 0}:] New topics added to target list: users
I, [2019-04-07T10:30:43.806083 #83821]  INFO -- : [[karafka_example_example] {users: 0}:] Fetching cluster metadata from kafka://127.0.0.1:9092
I, [2019-04-07T10:30:43.807502 #83821]  INFO -- : [[karafka_example_example] {users: 0}:] Discovered cluster metadata; nodes: your-machine-name.local:9092 (node_id=0)
</code></p>

<p>And let&rsquo;s send some dummy event from the console using <code>UsersResponder</code>:</p>

<p><code>text
rails console
</code></p>

<p><code>rb
UsersResponder.call({ event_name: "user_created", payload: { id: 1 } })
</code></p>

<p>The output of that command should be similar to the following one:
<code>text
I, [2019-04-07T10:33:31.216585 #84497]  INFO -- : New topics added to target list: users
I, [2019-04-07T10:33:31.216940 #84497]  INFO -- : Fetching cluster metadata from kafka://localhost:9092
D, [2019-04-07T10:33:31.217337 #84497] DEBUG -- : [topic_metadata] Opening connection to localhost:9092 with client id delivery_boy...
D, [2019-04-07T10:33:31.220633 #84497] DEBUG -- : [topic_metadata] Sending topic_metadata API request 1 to localhost:9092
D, [2019-04-07T10:33:31.221051 #84497] DEBUG -- : [topic_metadata] Waiting for response 1 from localhost:9092
D, [2019-04-07T10:33:31.222211 #84497] DEBUG -- : [topic_metadata] Received response 1 from localhost:9092
I, [2019-04-07T10:33:31.222554 #84497]  INFO -- : Discovered cluster metadata; nodes: your-machine-name.local:9092 (node_id=0)
D, [2019-04-07T10:33:31.222818 #84497] DEBUG -- : Closing socket to localhost:9092
D, [2019-04-07T10:33:31.227719 #84497] DEBUG -- : Current leader for users/0 is node your-machine-name.local:9092 (node_id=0)
I, [2019-04-07T10:33:31.228077 #84497]  INFO -- : Sending 1 messages to your-machine-name.local:9092 (node_id=0)
D, [2019-04-07T10:33:31.228535 #84497] DEBUG -- : [produce] Opening connection to your-machine-name.local:9092 with client id delivery_boy...
D, [2019-04-07T10:33:31.445056 #84497] DEBUG -- : [produce] Sending produce API request 1 to your-machine-name.local:9092
D, [2019-04-07T10:33:31.452673 #84497] DEBUG -- : [produce] Waiting for response 1 from your-machine-name.local:9092
D, [2019-04-07T10:33:31.507604 #84497] DEBUG -- : [produce] Received response 1 from your-machine-name.local:9092
D, [2019-04-07T10:33:31.507995 #84497] DEBUG -- : Successfully appended 1 messages to users/0 on your-machine-name.local:9092 (node_id=0)
 =&gt; {"users"=&gt;[["{\"event_name\":\"user_created\",\"payload\":{\"id\":1}}", {:topic=&gt;"users"}]]}
</code></p>

<p>And if you check the logs of the karafka server, you will see that the message was successfully processed, including logging the message:</p>

<p><code>text
I, [2019-04-07T10:33:33.348308 #83821]  INFO -- : [[karafka_example_example] {}:] New [User] event: {"create_time"=&gt;2019-04-07 10:33:31 +0200, "headers"=&gt;{}, "is_control_record"=&gt;false, "key"=&gt;nil, "offset"=&gt;0, "deserializer"=&gt;#&lt;Karafka::Serialization::Json::Deserializer:0x00007fd463e96dc8&gt;, "partition"=&gt;0, "receive_time"=&gt;2019-04-07 10:33:33 +0200, "topic"=&gt;"users", "payload"=&gt;{"event_name"=&gt;"user_created", "payload"=&gt;{"id"=&gt;1}}, "deserialized"=&gt;true}
I, [2019-04-07T10:33:33.348375 #83821]  INFO -- : [[karafka_example_example] {}:] Inline processing of topic users with 1 messages took 9 ms
I, [2019-04-07T10:33:33.348410 #83821]  INFO -- : [[karafka_example_example] {}:] 1 message on users topic delegated to UsersConsumer
</code></p>

<p>And that&rsquo;s it! That&rsquo;s all that it takes to use Kafka in your Rails application.</p>

<h3>What about other features?</h3>

<p>We only used a tiny part of the framework, which turned out to be sufficient for the demonstration purposes. However, there are <a href="https://github.com/karafka/karafka/wiki">way more features</a> that would be worth getting familiar with before using <code>karafka</code> in your application.</p>

<h2>Summary</h2>

<p>Kafka is a <strong>great tool</strong> if you want a <strong>reliable</strong> and <strong>performant</strong> way of <strong>sending events between applications</strong> where a <strong>persistent storage</strong> for a certain amount of time and an ability of <strong>replaying events</strong> is essential and/or if you plan to do heavy <strong>stream-processing</strong>. Thanks to frameworks like <a href="https://github.com/karafka/karafka">karafka</a>, it&rsquo;s quite simple to start using Kafka in Rails applications.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Messages on Rails Part 1 -  Introduction to Kafka and RabbitMQ]]></title>
    <link href="https://karolgalanciak.com/blog/2019/02/24/messages-on-rails-part-1-introduction-to-kafka-and-rabbitmq/"/>
    <updated>2019-02-24T22:00:00+01:00</updated>
    <id>https://karolgalanciak.com/blog/2019/02/24/messages-on-rails-part-1-introduction-to-kafka-and-rabbitmq</id>
    <content type="html"><![CDATA[<p><strong>Microservices</strong>, <strong>Service-Oriented Architecture (SOA)</strong> and in general, <strong>distributed ecosystems</strong>, have been on hype in the last several years. And that&rsquo;s for a good reason! At certain point, The <strong>Majestic Monolith</strong> &ldquo;pattern&rdquo; might start causing issues, both from the purely technical reasons like scalability, tight coupling of the code if you don&rsquo;t follow <strong>Domain-Driven Design</strong> or some other practices <strong>improving modularity</strong>, maintenance overhead, and also from organizational perspective since working in smaller teams on smaller apps is more efficient than working with huge team on an even bigger monolith which suffers from tight coupling and low cohesion. However, this is only true if the overall architecture addresses the potential problems that are common in the micro/macro-services world. One of these problems I would like to focus on is communication between apps and how the data flows between them.</p>

<!--more-->


<h2>How Can The Applications Talk To Each Other</h2>

<p>There are multiple ways the applications could talk to each other but let&rsquo;s focus on the most popular ones: shared database, HTTP API and messages.</p>

<h3>Shared Database</h3>

<p>It is arguable if we can talk about apps communicating with each other in this scenario; nevertheless, it&rsquo;s one of the ways to solve the problem of getting data from one application to another. And apart from some particular scenarios, it&rsquo;s the way that could give you a lot of headaches quite soon.</p>

<p>Initially, it might look like a great idea &ndash; you connect the other app to an existing database, and you don&rsquo;t need to worry about the communication! If the other application is just responsible for reading from that database, maybe you won&rsquo;t have any severe problems beyond increased load, issues of scaling and questionable access control (by default, the other app can do anything to that database). However, if you have multiple applications writing to the same database, you will undoubtedly have more serious issues &ndash; deadlocks, problems with data consistency as different constraints might apply to different applications, schema getting out of control with each application adding some custom tables and fields, which can turn out to be quite nasty &ndash; imagine one application acquiring Access Exclusive Lock on the entire table which turns out to be large (so the lock will not be released any time soon) thanks to some migration, while the other app was supposed to do some business-critical operations on that table that were also time-sensitive. Some of these problems are RDBMS-specific like locking during migration, but there is a very good chance anyway that you will be using either MySQL or PostgreSQL.</p>

<p>There are just too many things that can go wrong with having shared database that I would highly discourage from even trying that approach, maybe besides one single scenario &ndash; when what you need is exactly a replication of some data, e.g., to generate reports. In that case, you can set up PostgreSQL logical replication and let it replicate the data from the primary data source to a read-only replica containing only the data the application needs. Since PostgreSQL has a very reliable replication mechanism, you don&rsquo;t need to reinvent the wheel, and you can take advantage of that feature. Keep in mind that logical replication was introduced in PostgreSQL 10, so it&rsquo;s not available in the previous versions.</p>

<h3>HTTP API</h3>

<p>HTTP APIs, especially the RESTful APIs (although GraphQL has been getting quite popular in the last years), are arguably the most common blocks in microservices architecture as far as the communication between applications goes. REST architecture goes very well with HTTP, the communication between client and server is synchronous which makes it easy to reason about and debug, HTTP APIs are ubiquitous, and the developers are familiar with working with them. Also, HATEOAS and capabilities of getting precisely what you need from GraphQL make using with HTTP APIs easy, especially when working with Third-Party applications. Also, there is a lot of flexibility in implementing access control and authentication. On top of that, there are <a href="https://jsonapi.org">specifications</a> that clearly define how JSON APIs should be built, which minimizes reinventing the wheel and makes it even easier to build a client for that kind of API.</p>

<p>Based on those advantages it might look like communication between services via JSON APIs is the best possible solution to the problem, right?</p>

<p>Well, not necessarily. If you have one or two applications communicating with some other application, maybe it&rsquo;s not a bad idea, especially if the traffic is not huge and you don&rsquo;t need to transfer gigabytes of JSON daily via HTTP.</p>

<p>However, what if you expect a massive load from the services and indeed you will be transferring gigabytes of JSON via HTTP? Do you really need flexible authorization for communication between internal services? And why deal with an extra overhead of non-persistent connections and SSL handshakes in the first place?</p>

<p>Those things are great for integration with Third-Party applications, but they bring very little value in a closed ecosystem. Nevertheless, that overhead is not the biggest problem here, let&rsquo;s discuss something way more problematic: scaling.</p>

<p>Imagine that the API of one application is exposed to multiple other applications in your ecosystem. Also, the same application is used by &ldquo;real&rdquo; users who interact with it via UI. The immediate problem we have with that approach is that the traffic generated by the other services will be affecting regular users, which makes scaling decisions more tricky. To make it more complicated, let&rsquo;s say that in our example ecosystem we have a lot of data, and each application will be reading gigabytes of data every day and each service fetches data periodically from the API, e.g., every 5 minutes. To limit the number of requests so that we don&rsquo;t fetch the data that has been already fetched by the applications, we may want to implement some sort of data offset, which most likely will be based on the timestamp of the last fetch. We could send this timestamp that will filter out the records that haven&rsquo;t changed after that time and drastically reduce the number of requests that way. However, in a big enough scale, that will not be enough to solve all the issues. There can be thousands of records to be fetched every 5 minutes, which will most likely be paginated &ndash; that means a lot of requests. Multiply it by the number of applications and the traffic looks even more depressing, especially if every application will fetch exactly the same data.</p>

<p>Either the answer to the problem will be caching, which might be tricky to implement since the services will be using timestamp-based offset, or we can switch from pull-based strategy to push-based strategy.</p>

<p>A push-based strategy will be a publish-subscribe pattern via HTTP. The benefit of that is that we could serialize the state of the record after every change only once (or just serialize a changeset and go in the direction of CQRS/Event Sourcing) and send the payload to every service subscribed to a given event. The benefit of this approach is serializing a payload only once instead of doing it on every request, we can avoid useless requests which might happen if nothing has changed in the last 5 minutes and we could balance the load efficiently by moving the delivery logic to some background jobs. The disadvantage of publish-subscribe via HTTP, a.k.a. webhooks, would be implementing HTTP APIs in every service. Also, each application will experience massive traffic on a large enough scale.</p>

<p>It&rsquo;s great if your ecosystem offers this kind of features for limiting the number of requests to Third-Party Applications, but is it the most efficient way of handling communication between internal applications and scaling the ecosystem?</p>

<p>This type of communication is arguably quite common in the microservices world, and the funny thing is that in some aspects it&rsquo;s no different than reimplementing messaging systems, but done in a less effective way.</p>

<p>There is a more efficient way to deal with the communication between internal services: using asynchronous messaging.</p>

<h3>Asynchronous Messaging</h3>

<p>Async messaging is a communication approach where a middleware, called message broker, connects producers of messages with consumers of those messages. Usually, a producer sends messages and the message broker is responsible for delivering them to proper consumers.</p>

<p>Thanks to having a message broker in-between the services, not only do we benefit from extra decoupling between the applications, but also we have way more flexibility as far as scalability goes.</p>

<p>Let&rsquo;s take a look at two very popular message brokers that take different approaches to messages: <a href="https://www.rabbitmq.com">RabbitMQ</a> and <a href="http://kafka.apache.org">Apache Kafka</a></p>

<h4>RabbitMQ</h4>

<p>RabbitMQ is a general purpose message broker supporting multiple protocols from which AMQP will be the one most interesting one to us. It implements a smart broker/dumb consumer model, which means the broker is responsible for delivering messages to the right place.</p>

<p>The essential design decision in RabbitMQ is that the producers shouldn&rsquo;t directly push messages to the queues, from which the consumers can read messages and process them. Instead, producers send messages to exchanges, and the queues are connected to these exchanges. That way, the producer doesn&rsquo;t know where exactly the message will be delivered and how many consumers are going to do anything with it if any! This kind of decoupling not only allows implementation of the publish-subscribe pattern, where multiple consumers using different queues bind to the same exchange and process the same message, but it also opens the way to quite a flexible routing (which we will cover in more details in the next part of this series). Once the consumer acknowledges the message, it is removed from the queue. Multiple producers can also subscribe to the same queue which will make them compete for the messages. The communication in a simple scenario as described above is illustrated in the following diagram:</p>

<p class="img-center-wrapper">
  <img class="img-center" src="/images/messages_on_rails_part_1/rabbitmq.png" title="RabbitMQ" alt="RabbitMQ">
</p>




<p class="center">
  <a href="https://karolgalanciak.com/images/messages_on_rails_part_1/rabbitmq.png" target="_blank">See in better quality</a>
</p>


<p>RabbitMQ is an excellent choice if you have complex routing scenarios, priority queues, you don&rsquo;t care about keeping messages in the queue after they are processed, and you don&rsquo;t expect an extreme throughput (although 100k messages/s that you can get in some <a href="https://www.cloudamqp.com/plans.html">RabbitMQ As A Service solutions</a>, even in a high availability scenario, is in most cases more than enough).</p>

<h4>Kafka</h4>

<p>Kafka is a distributed streaming platform which takes a different approach than RabbitMQ &ndash; it implements a dumb broker/smart consumer model. In that case,  it is the responsibility of the consumers to fetch messages and keep track of their state. What is unique about Kafka is the fact that it is not a message queue &ndash; it is an append-only log. Producers send messages to the topics, to which consumers subscribe and read the messages and keep the offset which is the information about the position in the log until which the messages were read. It can be illustrated the following way:</p>

<p><img src="/images/messages_on_rails_part_1/kafka.png" title="&lsquo;Kafka&rsquo; &lsquo;Kafka&rsquo;" ></p>

<p class="center">
  <a href="https://karolgalanciak.com/images/messages_on_rails_part_1/kafka.png" target="_blank">See in better quality</a>
</p>


<p>Such a design opens a way to some interesting features. The log is persistent with configurable retention which can be based on the size or time how long the events should be kept in the log, which makes Kafka act like a circular buffer. To save some space, Kafka allows log compaction when the messages are sent with the same key, keeping only the last one as a result. If storage is not an issue, you might even decide never to delete any messages!</p>

<p>On top of that, Kafka can easily handle a massive throughput &ndash; 100 000 messages/second is not extraordinary, it&rsquo;s easy to scale, and there are <a href="https://events.static.linuxfound.org/sites/events/files/slides/Kafka%20At%20Scale.pdf">cases</a> of handling millions of messages per second.</p>

<p>A persistent log is a killer-feature of Kafka. When you add another service to your ecosystem and need to get the data from one app to another, you will run into obvious issues when using a traditional message queue since the messages are gone after they are processed. With Kafka, just can add a new consumer and replay all the past events! You can also combine Kafka with stream processing frameworks like <a href="http://samza.apache.org">Apache Samza</a>, <a href="http://spark.apache.org">Apache Spark</a>, <a href="http://storm.apache.org">Apache Storm</a>,<a href="https://www.confluent.io/product/ksql/">KSQL</a> or use Kafka Streams.</p>

<p>Kafka is an excellent choice for use cases where the routing is not complex, the throughput is substantial, the messages need to stay in the log (even indefinitely) and where you expect the need to replay messages. You can also do Event Sourcing with Kafka and use stream processor like Kafka Stream or KSQL to construct projections from the events.</p>

<h2>Summary</h2>

<p><strong>Communication</strong> between multiple services in a <strong>distributed ecosystem</strong> is far from a trivial problem, which can be approached in various ways: via a <strong>shared database</strong> (although to a very limited extent), <strong>HTTP APIs</strong> or <strong>messages</strong>.</p>

<p>In the next parts I&rsquo;ll be focusing on <strong>Kafka</strong> and <strong>RabbitMQ</strong> &ndash; how they work and how to use them in Rails applications, how to produce and consume messages, what kind of extra options you get by introducing Kafka and RabbitMQ to your ecosystem, potential issues you might have in some specific scenarios and show some examples where combining both could bring benefits.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to tell the difference between a default and a provided value for optional arguments in Ruby?]]></title>
    <link href="https://karolgalanciak.com/blog/2019/01/27/how-to-tell-the-difference-between-a-default-and-provided-value-for-optional-arguments-in-ruby/"/>
    <updated>2019-01-27T20:00:00+01:00</updated>
    <id>https://karolgalanciak.com/blog/2019/01/27/how-to-tell-the-difference-between-a-default-and-provided-value-for-optional-arguments-in-ruby</id>
    <content type="html"><![CDATA[<p>It is sometimes required for the methods with optional arguments to be able to differentiate between its default value and the value passed from the caller. Passing <code>nil</code> might initially sound like a good idea since it represents &ldquo;nothingness&rdquo;. However, it might turn out that <code>nil</code> is a legit value and there might be cases where it is desirable for the caller to pass <code>nil</code>. In such a case, we cannot use it as a default value if we want to implement a special logic for the case of not providing that value.</p>

<p>Fortunately, there is an easy way to deal with it &ndash; use a special constant:</p>

<p>``` rb
class SomeClass
  NO_VALUE_PROVIDED = Object.new
  private_constant :NO_VALUE_PROVIDED</p>

<p>  def call(argument: NO_VALUE_PROVIDED)</p>

<pre><code>if argument == NO_VALUE_PROVIDED
  handle_scenario_for_no_value_provided
else
  do_something_with_argument(argument)
end
</code></pre>

<p>  end
end
```</p>

<p>In <code>call</code> method, we allow to pass an optional argument with a default of <code>NO_VALUE_PROVIDED</code>, which is a private constant defined in that class that is an instance of <code>Object</code>.</p>

<p>By depending on the instance of <code>Object</code> that is initialized inside that class, we can avoid cases where the equality check returns <code>true</code> even if this is not an expected outcome, which could happen if we used strings or symbols. We could use some symbol that would be very unlikely to be passed from the caller, like <code>:__no_value_provided__,</code> but it arguably looks more like a workaround than a dedicated solution for the problem.</p>

<p>Also, a private constant ensures it is not used anywhere outside the class, which minimizes the chances that the passed argument would the same as our placeholder for no-value-provided scenario even more.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Inheritance and define_method - how to make them work together]]></title>
    <link href="https://karolgalanciak.com/blog/2018/12/16/inheritance-and-define-method-how-to-make-them-work-together/"/>
    <updated>2018-12-16T20:00:00+01:00</updated>
    <id>https://karolgalanciak.com/blog/2018/12/16/inheritance-and-define-method-how-to-make-them-work-together</id>
    <content type="html"><![CDATA[<p>Imagine that you are implementing some form object because you are fed up with treating ActiveRecord models as such, and you need some extra flexibility. You start with a straightforward implementation for a base class of a form object where you can just whitelist attributes. That could look like this:</p>

<p>``` rb
class FormObject
  def self.attributes_registry</p>

<pre><code>@attributes_registry ||= []
</code></pre>

<p>  end</p>

<p>  def self.attribute(attribute_name)</p>

<pre><code>attributes_registry &lt;&lt; attribute_name

define_method(attribute_name) do
  instance_variable_get("@#{attribute_name}")
end

define_method("#{attribute_name}=") do |value|
  instance_variable_set("@#{attribute_name}", value)
end
</code></pre>

<p>  end
end
```</p>

<p>Since the base class is ready, you can create a first form object that would inherit from this class:</p>

<p><code>rb
class MyForm &lt; FormObject
  attribute :some_attribute
end
</code></p>

<p>Initially, it does the job, but then it turns out that you might need a default value if <code>some_attribute</code> turns out to be nil. So you try something like that:</p>

<p>``` rb
class MyFormWithDefaultValue &lt; FormObject
  attribute :some_attribute</p>

<p>  def some_attribute</p>

<pre><code>super || "Default"
</code></pre>

<p>  end
end
```</p>

<p>After checking if the default value works, this is what you get:</p>

<p>```</p>

<blockquote><p>MyFormWithDefaultValue.new.some_attribute
=> NoMethodError: super: no superclass method `some_attribute' for #&lt;MyFormWithDefaultValue:0x007f84a50ae8e0>
```</p></blockquote>

<p>Whoops! How did it happen? The method was defined in the superclass so it should be inheritable, right?</p>

<p>Well, this is not really true. However, the problem is easy to fix.</p>

<!--more-->


<h2>Anatomy Of The Problem</h2>

<p>The primary question we should answer in the first place is: where are all those new methods defined using <code>define_method</code> in that particular way? Is it a superclass?</p>

<p><code>
 FormObject.instance_methods - Object.methods
 =&gt; []
</code></p>

<p>It&rsquo;s definitely not a superclass &ndash; there are no any instance methods defined there, there are only the ones inherited from <code>Object</code>. What about <code>MyFormWithDefaultValue</code>?</p>

<p>```</p>

<blockquote><p>MyFormWithDefaultValue.instance_methods &ndash; Object.methods
 => [:some_attribute, :some_attribute=]
```</p></blockquote>

<p>Now the error that we initially got makes way more sense. The entire issue is caused by the fact that the declaration of the attribute happens in <code>MyFormWithDefaultValue</code>, if it were defined in a base class, there would be no any issue. We can verify it with a simple example:</p>

<p>``` rb
class MyForm &lt; FormObject
  attribute :some_attribute
end</p>

<p>class MyFormWithDefaultValue &lt; MyForm
  def some_attribute</p>

<pre><code>super || "Default"
</code></pre>

<p>  end
end
```</p>

<p>```</p>

<blockquote><p>MyFormWithDefaultValueA.new.some_attribute
 => &ldquo;Default&rdquo;
```</p></blockquote>

<h2>Solution</h2>

<p>Now that we fully understand the problem let&rsquo;s think about the solution. Ideally, about the one, that doesn&rsquo;t require defining explicitly an intermediate class that we can inherit from.</p>

<p>How about defining a module instead? Modules are also included in the inheritance chain, and for using <code>super</code>, it doesn&rsquo;t matter if the method is defined in the class or a module.</p>

<p>The exact solution to the problem would be wrapping the definition of new methods that happens in <code>FormObject</code> inside some module, it could be even an anonymous one, and including it right away:</p>

<p>```
class FormObject
  def self.attributes_registry</p>

<pre><code>@attributes_registry ||= []
</code></pre>

<p>  end</p>

<p>  def self.attribute(attribute_name)</p>

<pre><code>attributes_registry &lt;&lt; attribute_name

wrapper = Module.new do
  define_method(attribute_name) do
    instance_variable_get("@#{attribute_name}")
  end

  define_method("#{attribute_name}=") do |value|
    instance_variable_set("@#{attribute_name}", value)
  end
end
include wrapper
</code></pre>

<p>  end
end
```</p>

<p>Let&rsquo;s verify if the new solution works:</p>

<p>```</p>

<blockquote><p>MyFormWithDefaultValue.new.some_attribute
 => &ldquo;Default&rdquo;</p></blockquote>

<p>```</p>

<p>Yay! It does exactly what we wanted to achieve.</p>

<h2>Wrapping Up</h2>

<p>Metaprogramming in Ruby is a powerful tool, however; it can lead to some issues that might not be obvious why they happen. Fortunately, with enough knowledge of the language, those problems can be solved elegantly.</p>
]]></content>
  </entry>
  
</feed>
